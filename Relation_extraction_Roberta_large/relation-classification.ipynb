{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport torch\nfrom torch import nn\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.cuda import amp\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-21T08:57:03.300264Z","iopub.execute_input":"2022-05-21T08:57:03.301118Z","iopub.status.idle":"2022-05-21T08:57:03.313773Z","shell.execute_reply.started":"2022-05-21T08:57:03.300955Z","shell.execute_reply":"2022-05-21T08:57:03.31314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing part\nThe dataset provided here is already labeled the concept, so what we need to do here: classification ","metadata":{}},{"cell_type":"markdown","source":"Processing step:\n1. read all the content in\n2. divide into train","metadata":{}},{"cell_type":"code","source":"def read_dataset(path):\n    with open(path, \"r\") as f:\n        lines = f.readlines()\n        content_ls = []\n        label_ls_d = []\n        comment_ls = []\n        for ids in range(0,len(lines),4):\n            content_ls.append(lines[ids])\n            label_ls_d.append(lines[ids+1].split(\"\\n\")[0])\n            comment_ls.append(re.sub(\"Comment:\",\"\",lines[ids+2].split(\"\\n\")[0]))\n        assert len(label_ls_d) == len(content_ls) == len(comment_ls)\n        ids_ls = []\n        concept_ls = []\n        new_content = []\n        for content in content_ls:\n            ids = int(content.split(\"\\t\")[0])\n            Cause = content.split(\"<e1>\")[1].split(\"</e1>\")[0]\n            Effect = content.split(\"<e2>\")[1].split(\"</e2>\")[0]\n            content_ = re.sub(\"<e1>\",\"\",content.split(\"\\t\")[1].split(\"\\n\")[0])\n            content_ = re.sub(\"</e1>\",\"\",content_)\n            content_ = re.sub(\"<e2>\",\"\",content_)\n            content_ = re.sub(\"</e2>\",\"\",content_)\n            content_ = re.sub('\"',\"\",content_)\n            new_content.append(content_)\n            ids_ls.append(ids)\n            concept_ls.append((Cause,Effect))\n        assert len(label_ls_d) == len(concept_ls) == len(comment_ls) == len(ids_ls) == len(new_content)\n    return ids_ls, concept_ls, comment_ls,new_content ,label_ls_d","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.315685Z","iopub.execute_input":"2022-05-21T08:57:03.31609Z","iopub.status.idle":"2022-05-21T08:57:03.329813Z","shell.execute_reply.started":"2022-05-21T08:57:03.31606Z","shell.execute_reply":"2022-05-21T08:57:03.329133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = \"../input/dpw3-project-dataset/DPW3-RelationExtraction/TRAIN_FILE.TXT\"\nvalid_path = \"../input/dpw3-project-dataset/DPW3-RelationExtraction/FULL_TEST.txt\"\ntrain_ids, train_concepts, train_comments, train_content, train_labels = read_dataset(train_path)\nvalid_ids, valid_concepts, valid_comments, valid_content, valid_labels = read_dataset(valid_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.331141Z","iopub.execute_input":"2022-05-21T08:57:03.331587Z","iopub.status.idle":"2022-05-21T08:57:03.508233Z","shell.execute_reply.started":"2022-05-21T08:57:03.331558Z","shell.execute_reply":"2022-05-21T08:57:03.507192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ids[0], train_concepts[0], train_content[0], train_comments[0], train_labels[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.510058Z","iopub.execute_input":"2022-05-21T08:57:03.510622Z","iopub.status.idle":"2022-05-21T08:57:03.519746Z","shell.execute_reply.started":"2022-05-21T08:57:03.510583Z","shell.execute_reply":"2022-05-21T08:57:03.519061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check max length","metadata":{}},{"cell_type":"markdown","source":"## Here I use roberta-large for test\nI will process this task as a QA problem, which takes the following schedule:\n1. question: [content]\n2. answer: answer_concepts\n\nAnd the final input will be [question; answer]","metadata":{}},{"cell_type":"code","source":"# not use comment ls since no provided from test data\ndef convert_input(concept_ls, content_ls, label_ls, comment_ls,label_mapping,model_name = \"roberta-large\", max_length = 128):\n    label_ls = [label_mapping[i] for i in label_ls]\n    tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n    sep = [tokenizer.sep_token, tokenizer.sep_token]\n    input_ids_ls, seg_ids_ls, att_mask_ls = [], [], []\n    for content, answer_concept, comment in tqdm(zip(content_ls,concept_ls, comment_ls)):\n        ans_tk = answer_concept[0]+\" \"+answer_concept[1]+\" \"+comment\n        answer_tokens = tokenizer.tokenize(ans_tk)\n        question_tokens = tokenizer.tokenize(content)\n        while True:\n            total_length = len(answer_tokens) + len(question_tokens)\n            if total_length <= max_length - 4:\n                break\n            if len(question_tokens) >= len(answer_tokens):\n                question_tokens.pop()\n            else:\n                answer_tokens.pop()\n        tokens_a = [tokenizer.cls_token]+answer_tokens + sep\n        tokens_b = question_tokens + [tokenizer.sep_token]\n        all_tokens = tokens_a + tokens_b\n        seg_ids = [0]*len(all_tokens)\n        input_ids = tokenizer.convert_tokens_to_ids(all_tokens)\n        input_mask = [1] * len(input_ids)\n        # padding\n        padding_length = max_length - len(input_ids)\n        input_ids = input_ids + [0] * padding_length\n        seg_ids = seg_ids + [0] * padding_length\n        input_mask = input_mask + [0] * padding_length\n        input_ids_ls.append(input_ids)\n        seg_ids_ls.append(seg_ids)\n        att_mask_ls.append(input_mask)\n    assert len(input_ids_ls) == len(att_mask_ls) == len(seg_ids_ls)\n    return input_ids_ls, att_mask_ls, seg_ids_ls, label_ls","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.521993Z","iopub.execute_input":"2022-05-21T08:57:03.522276Z","iopub.status.idle":"2022-05-21T08:57:03.537723Z","shell.execute_reply.started":"2022-05-21T08:57:03.522244Z","shell.execute_reply":"2022-05-21T08:57:03.537033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_input_ids, train_att_mask, train_seg_ids, train_labels = convert_input(train_concepts, train_content,train_labels, label_mapping)\n# valid_input_ids, valid_att_mask, valid_seg_ids, valid_labels = convert_input(valid_concepts, valid_content,valid_labels, label_mapping)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.539179Z","iopub.execute_input":"2022-05-21T08:57:03.539557Z","iopub.status.idle":"2022-05-21T08:57:03.558458Z","shell.execute_reply.started":"2022-05-21T08:57:03.539412Z","shell.execute_reply":"2022-05-21T08:57:03.557575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Relation_cls(torch.utils.data.Dataset):\n    def __init__(self, input_ids, att_mask, seg_ids, labels):\n        self.input_ids = input_ids\n        self.att_mask = att_mask\n        self.seg_ids = seg_ids\n        self.labels = labels\n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, ids):\n        return torch.tensor(self.input_ids[ids], dtype = torch.long), torch.tensor(self.att_mask[ids], dtype =torch.long), torch.tensor(self.seg_ids[ids], dtype = torch.long),  torch.tensor(self.labels[ids], dtype = torch.long)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.559703Z","iopub.execute_input":"2022-05-21T08:57:03.560311Z","iopub.status.idle":"2022-05-21T08:57:03.572868Z","shell.execute_reply.started":"2022-05-21T08:57:03.560271Z","shell.execute_reply":"2022-05-21T08:57:03.572037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_set = Relation_cls(train_input_ids, train_att_mask,train_seg_ids,train_labels)\n# valid_set = Relation_cls(valid_input_ids, valid_att_mask,valid_seg_ids,valid_labels)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.574174Z","iopub.execute_input":"2022-05-21T08:57:03.574935Z","iopub.status.idle":"2022-05-21T08:57:03.59248Z","shell.execute_reply.started":"2022-05-21T08:57:03.574891Z","shell.execute_reply":"2022-05-21T08:57:03.591567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Text_model(nn.Module):\n    def __init__(self, model_name,path = None, num_class = 19):\n        super().__init__()\n        if path is None:\n            # download model from website\n            self.model = AutoModel.from_pretrained(model_name)\n        else:\n            self.model =AutoModel.from_pretrained(path)\n        self.output_size = self.model.config.hidden_size\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Linear(self.output_size, num_class)\n    \n    def forward(self, input_ids, seg_ids, att_mask):\n        output = self.model(input_ids, attention_mask = att_mask, token_type_ids = seg_ids)\n        pooled_output = output[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.595093Z","iopub.execute_input":"2022-05-21T08:57:03.595575Z","iopub.status.idle":"2022-05-21T08:57:03.606393Z","shell.execute_reply.started":"2022-05-21T08:57:03.595523Z","shell.execute_reply":"2022-05-21T08:57:03.605697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Text_model(\"roberta-large\")","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.607842Z","iopub.execute_input":"2022-05-21T08:57:03.608376Z","iopub.status.idle":"2022-05-21T08:57:03.619284Z","shell.execute_reply.started":"2022-05-21T08:57:03.608335Z","shell.execute_reply":"2022-05-21T08:57:03.618598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_iter = torch.utils.data.DataLoader(train_set, batch_size = 2)\n# input_ids, att_mask, seg_ids,label = next(iter(train_iter))\n# model(input_ids, seg_ids, att_mask)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.620662Z","iopub.execute_input":"2022-05-21T08:57:03.620932Z","iopub.status.idle":"2022-05-21T08:57:03.634378Z","shell.execute_reply.started":"2022-05-21T08:57:03.620893Z","shell.execute_reply":"2022-05-21T08:57:03.633447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_accuracy(eval_set, model, criterion,device):\n    model.to(device)\n    n_samples, n_correct = 0, 0\n    model.eval()\n    loss_ls = []\n    with torch.no_grad():\n        for idx, value in enumerate(tqdm(eval_set)):\n            input_ids, att_mask ,seg_ids,labels = value\n            input_ids   = input_ids.to(device)\n            att_mask    = att_mask.to(device)\n            labels      = labels.to(device)\n            seg_ids     = seg_ids.to(device)\n            with amp.autocast(enabled= True):\n                logits = model(input_ids,seg_ids, att_mask)  \n            loss        = criterion(logits, labels)\n            n_correct += (logits.argmax(1) == labels).sum().item()\n            n_samples += labels.size(0)\n            loss_ls.append(loss.item())\n            #print(\"pass\")\n    return n_correct / n_samples, loss_ls","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.635615Z","iopub.execute_input":"2022-05-21T08:57:03.636033Z","iopub.status.idle":"2022-05-21T08:57:03.647539Z","shell.execute_reply.started":"2022-05-21T08:57:03.635991Z","shell.execute_reply":"2022-05-21T08:57:03.646839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_with_amp(net, train_set, valid_set,criterion, optimizer, epochs,batch_size, scheduler, gradient_accumulate_step, max_grad_norm , num_gpu):\n    net.train()   \n    \n    # instantiate a scalar object \n    ls          = []\n    #device_ids  = [try_gpu(i) for i in range(num_gpu)]\n    device  = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(\"\\ntrain on %s\\n\"%str(device))\n    enable_amp  = True if \"cuda\" in device.type else False\n    scaler      = amp.GradScaler(enabled= enable_amp)\n    net.to(device)\n    train_iter  = torch.utils.data.DataLoader(train_set, batch_size = batch_size,shuffle = True)\n    valid_iter = torch.utils.data.DataLoader(valid_set, batch_size = batch_size)\n    for epoch in range(epochs):\n        net.train()\n        for idx, value in enumerate(train_iter):\n            input_ids, att_mask ,seg_ids,labels = value\n            input_ids   = input_ids.to(device)\n            att_mask    = att_mask.to(device)\n            labels      = labels.to(device)\n            seg_ids     = seg_ids.to(device)\n            # when forward process, use amp\n            with amp.autocast(enabled= enable_amp):\n                output  = net(input_ids, seg_ids, att_mask)  \n            loss        = criterion(output, labels)\n            # prevent gradient to 0\n            if gradient_accumulate_step > 1:\n                # 如果显存不足，通过 gradient_accumulate 来解决\n                loss    = loss/gradient_accumulate_step\n            \n            # 放大梯度，避免其消失\n            scaler.scale(loss).mean().backward()\n            # do the gradient clip\n            gradient_norm = nn.utils.clip_grad_norm_(net.parameters(),max_grad_norm)\n            if (idx + 1) % gradient_accumulate_step == 0:\n                # 多少 step 更新一次梯度\n                # 通过 scaler.step 来unscale 回梯度值， 如果气结果不是infs 和Nans， 调用optimizer.step()来更新权重\n                # 否则忽略step调用， 保证权重不更新\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n            # 每1000次计算 print 出一次loss\n            if idx % 30 == 0 or idx == len(train_iter) -1:\n                with torch.no_grad():\n                    print(\"==============Epochs \"+ str(epoch) + \" ======================\")\n                    print(\"loss: \" + str(loss) + \"; grad_norm: \" + str(gradient_norm))\n                ls.append(loss.item())\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': net.state_dict(),\n                    'param_groups': optimizer.state_dict()[\"param_groups\"],\n                    'loss': ls\n                },\"./checkpoint.params\")\n                \n        print(\"start evaluation:...\")\n        acc, loss_ls = evaluate_accuracy(valid_iter, net, criterion,device)\n        prediction_ls, labels_ls = prediction(valid_iter, net, device)\n        pred = torch.cat(prediction_ls).cpu().numpy()\n        labels_true = torch.cat(labels_ls).cpu().numpy()\n        print(\"Micro:\", f1_score(labels_true, pred, average=\"micro\"))\n        print(\"Macro:\", f1_score(labels_true, pred, average=\"macro\"))\n        print(\"acc: \", acc)\n        with open(\"train_log\", \"a\") as f:\n                f.write(\"Epoch %s, eval_accuracy %.4f, valid_loss %s, Macro %.4f, Micro %.4f \\n\"%(epoch, acc, loss_ls,f1_score(labels_true, pred, average=\"macro\"),\n                                                                                                 f1_score(labels_true, pred, average=\"micro\")))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.649241Z","iopub.execute_input":"2022-05-21T08:57:03.649881Z","iopub.status.idle":"2022-05-21T08:57:03.670222Z","shell.execute_reply.started":"2022-05-21T08:57:03.649804Z","shell.execute_reply":"2022-05-21T08:57:03.669116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef prediction(eval_set, model,device):\n    model.to(device)\n    prediction_ls, label_ls = [], []\n    model.eval()\n    loss_ls = []\n    with torch.no_grad():\n        for idx, value in enumerate(tqdm(eval_set)):\n            input_ids, att_mask ,seg_ids,labels = value\n            input_ids   = input_ids.to(device)\n            att_mask    = att_mask.to(device)\n            labels      = labels.to(device)\n            seg_ids     = seg_ids.to(device)\n            with amp.autocast(enabled= True):\n                logits = model(input_ids,seg_ids, att_mask)  \n            predict = logits.argmax(1)\n            prediction_ls.append(predict)\n            label_ls.append(labels)\n            #print(\"pass\")\n    return prediction_ls, label_ls","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.672116Z","iopub.execute_input":"2022-05-21T08:57:03.672743Z","iopub.status.idle":"2022-05-21T08:57:03.690612Z","shell.execute_reply.started":"2022-05-21T08:57:03.672698Z","shell.execute_reply":"2022-05-21T08:57:03.68952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Macro: 0.78, Micro: 0.83 for epochs 5. Try 3 more epochs:\n\nMacro: 0.816, Micro: 0.85 for epochs 8. \n\nTry 2 more epochs with lower learning rate: 1e-7 and Adam optimizer: Macro: 0.823, Micro: 0.856","metadata":{}},{"cell_type":"markdown","source":"## Visulization","metadata":{}},{"cell_type":"markdown","source":"## Loss and accuracy","metadata":{}},{"cell_type":"code","source":"import re\ndef change_logs(path):\n    accuracies = []\n    valid_loss = []\n    #valid_micro = []\n    valid_macro = []\n    with open(path, \"r\") as f:\n        file = f.readlines()\n        #print(file)\n    for log_ in file:\n        accuracies.append(ast.literal_eval(log_.split(\"eval_accuracy \")[1][0:6]))\n        loss_ls = ast.literal_eval(log_.split(\"\\n\")[0].split(\"valid_loss \")[-1].split(\"Macro\")[0])[0]\n        valid_loss.append(np.mean(loss_ls))\n        #print(valid_loss, accuracies)\n        valid_macro.append(float(re.sub(\" \", \"\", log_.split(\"Macro\")[-1].split(\", \")[0])))\n#         break\n    return np.array(accuracies), np.array(valid_loss), np.array(valid_macro)\n#change_logs(\"../input/relation-cls-params/train_log\")","metadata":{"execution":{"iopub.status.busy":"2022-05-21T08:57:03.692507Z","iopub.execute_input":"2022-05-21T08:57:03.693152Z","iopub.status.idle":"2022-05-21T08:57:03.710243Z","shell.execute_reply.started":"2022-05-21T08:57:03.693107Z","shell.execute_reply":"2022-05-21T08:57:03.709512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Init train with 5 epochs, we can see the accuracy and valid loss shown below","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport ast\npath = \"../input/relation-cls-params/train_log\"\nvalid_acc, valid_loss, valid_Marco = change_logs(path)\nr_int = 0.05\nplt.figure()\nepochs = [\"epochs 0\", \"epochs 1\"]\nplt.plot(epochs, valid_acc , c = \"red\", label = \"accuracy\", linestyle = \"--\")\nplt.scatter(epochs, valid_acc  , c = \"red\")\nplt.scatter(epochs, valid_loss , c= \"blue\")\nplt.scatter(epochs, valid_Marco , c= \"Green\")\nplt.plot(epochs ,valid_loss, c=\"blue\", label = \"loss\")\nplt.plot(epochs ,valid_Marco, c=\"Green\", label = \"Marco\")\nplt.xlabel(\"epochs\")\nplt.title(\"Validation loss/accuracy per epochs/Marco\")\nplt.legend(loc = \"best\")\nplt.annotate(r\"acc:%.3f\"%(valid_acc[-1] ), xy = (epochs[-1], valid_acc[-1]), xytext = (-50,+30), textcoords = \"offset points\",fontsize = 14,\n            arrowprops=dict(arrowstyle='->',connectionstyle='arc3,rad=.2'))\nplt.annotate(r\"loss:%.3f\"%(valid_loss[-1]), xy = (epochs[-1], valid_loss[-1]), xytext = (+50,-30), textcoords = \"offset points\",fontsize = 12,\n            arrowprops=dict(arrowstyle='->',connectionstyle='arc3,rad=.2'))\nplt.annotate(r\"Macro-F1:%.3f\"%(valid_Marco[-1]), xy = (epochs[-1], valid_Marco[-1]), xytext = (-120,-40), textcoords = \"offset points\",fontsize = 12,\n            arrowprops=dict(arrowstyle='->',connectionstyle='arc3,rad=.2'))\nplt.savefig(\"./validation.png\")\n# plt.ylim(0,1)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T09:00:44.6507Z","iopub.execute_input":"2022-05-21T09:00:44.651022Z","iopub.status.idle":"2022-05-21T09:00:45.126451Z","shell.execute_reply.started":"2022-05-21T09:00:44.650991Z","shell.execute_reply":"2022-05-21T09:00:45.12516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eval","metadata":{}},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     train_path = \"../input/dpw3-project-dataset/DPW3-RelationExtraction/TRAIN_FILE.TXT\"\n#     valid_path = \"../input/dpw3-project-dataset/DPW3-RelationExtraction/FULL_TEST.txt\"\n#     train_ids, train_concepts, train_comments, train_content, train_labels = read_dataset(train_path)\n#     valid_ids, valid_concepts, valid_comments, valid_content, valid_labels = read_dataset(valid_path)\n#     label_mapping = {label:ids for ids, label in enumerate(np.unique(train_labels).tolist())}\n#     train_input_ids, train_att_mask, train_seg_ids, train_labels = convert_input(train_concepts, train_content, train_labels,train_comments, label_mapping)\n#     valid_input_ids, valid_att_mask, valid_seg_ids, valid_labels = convert_input(valid_concepts, valid_content,valid_labels,valid_comments, label_mapping)\n\n#     train_set = Relation_cls(train_input_ids, train_att_mask,train_seg_ids,train_labels)\n#     valid_set = Relation_cls(valid_input_ids, valid_att_mask,valid_seg_ids,valid_labels)\n#     batch_size = 16\n#     valid_iter = torch.utils.data.DataLoader(valid_set, batch_size = batch_size)\n#     model = Text_model(\"roberta-large\")\n#     dic = torch.load(\"../input/relation-cls-params/checkpoint.params\")\n#     model.load_state_dict(dic[\"model_state_dict\"])\n#     prediction_ls, labels_ls = prediction(valid_iter, model, torch.device(\"cuda\"))\n#     pred = torch.cat(prediction_ls).cpu().numpy()\n#     labels_true = torch.cat(labels_ls).cpu().numpy()\n#     print(\"Mirco score for final model:\", f1_score(labels_true, pred, average=\"micro\"))\n#     print(\"Macro score for final model:\", f1_score(labels_true, pred, average=\"macro\"))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T06:56:05.097437Z","iopub.execute_input":"2022-05-21T06:56:05.097631Z","iopub.status.idle":"2022-05-21T06:58:23.76272Z","shell.execute_reply.started":"2022-05-21T06:56:05.097608Z","shell.execute_reply":"2022-05-21T06:58:23.761803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train from initial","metadata":{}},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     train_path = \"../input/dpw3-project-dataset/DPW3-RelationExtraction/TRAIN_FILE.TXT\"\n#     valid_path = \"../input/dpw3-project-dataset/DPW3-RelationExtraction/FULL_TEST.txt\"\n#     train_ids, train_concepts, train_comments, train_content, train_labels = read_dataset(train_path)\n#     valid_ids, valid_concepts, valid_comments, valid_content, valid_labels = read_dataset(valid_path)\n#     label_mapping = {label:ids for ids, label in enumerate(np.unique(train_labels).tolist())}\n#     train_input_ids, train_att_mask, train_seg_ids, train_labels = convert_input(train_concepts, train_content, train_labels,train_comments, label_mapping)\n#     valid_input_ids, valid_att_mask, valid_seg_ids, valid_labels = convert_input(valid_concepts, valid_content,valid_labels,valid_comments, label_mapping)\n#     train_set = Relation_cls(train_input_ids, train_att_mask,train_seg_ids,train_labels)\n#     valid_set = Relation_cls(valid_input_ids, valid_att_mask,valid_seg_ids,valid_labels)\n#     criterion = nn.CrossEntropyLoss()\n#     batch_size = 16\n#     lr = 2e-6\n#     num_gpu = 1\n#     model = Text_model(\"roberta-large\")\n#     optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n#     scheduler = get_cosine_schedule_with_warmup(optimizer= optimizer, num_warmup_steps = 0, \n#                                                 num_training_steps= len(torch.utils.data.DataLoader(train_set, batch_size = batch_size)), num_cycles = 0.5)\n#     train_with_amp(model, train_set, valid_set, criterion, optimizer, epochs=6, batch_size = batch_size, scheduler=scheduler, gradient_accumulate_step=1,\n#                   max_grad_norm=1000, num_gpu=num_gpu)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T06:33:53.853714Z","iopub.execute_input":"2022-05-21T06:33:53.854073Z","iopub.status.idle":"2022-05-21T06:33:53.860064Z","shell.execute_reply.started":"2022-05-21T06:33:53.854027Z","shell.execute_reply":"2022-05-21T06:33:53.858683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine tune","metadata":{}},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     train_path = \"../input/dpw3-project-dataset/DPW3-RelationExtraction/TRAIN_FILE.TXT\"\n#     valid_path = \"../input/dpw3-project-dataset/DPW3-RelationExtraction/FULL_TEST.txt\"\n#     train_ids, train_concepts, train_comments, train_content, train_labels = read_dataset(train_path)\n#     valid_ids, valid_concepts, valid_comments, valid_content, valid_labels = read_dataset(valid_path)\n#     label_mapping = {label:ids for ids, label in enumerate(np.unique(train_labels).tolist())}\n#     train_input_ids, train_att_mask, train_seg_ids, train_labels = convert_input(train_concepts, train_content, train_labels,train_comments, label_mapping)\n#     valid_input_ids, valid_att_mask, valid_seg_ids, valid_labels = convert_input(valid_concepts, valid_content,valid_labels,valid_comments, label_mapping)\n\n#     train_set = Relation_cls(train_input_ids, train_att_mask,train_seg_ids,train_labels)\n#     valid_set = Relation_cls(valid_input_ids, valid_att_mask,valid_seg_ids,valid_labels)\n#     criterion = nn.CrossEntropyLoss()\n#     batch_size = 16\n#     lr = 1e-6\n#     num_gpu = 1\n#     model = Text_model(\"roberta-large\")\n#     dic = torch.load(\"../input/relation-cls-params/checkpoint.params\")\n#     model.load_state_dict(dic[\"model_state_dict\"])\n#     optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n#     scheduler = get_cosine_schedule_with_warmup(optimizer= optimizer, num_warmup_steps = 0, \n#                                                 num_training_steps= len(torch.utils.data.DataLoader(train_set, batch_size = batch_size)), num_cycles = 0.5)\n#     train_with_amp(model, train_set, valid_set, criterion, optimizer, epochs=2, batch_size = batch_size, scheduler=scheduler, gradient_accumulate_step=1,\n#                   max_grad_norm=1000, num_gpu=num_gpu)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T06:33:53.861661Z","iopub.execute_input":"2022-05-21T06:33:53.862327Z","iopub.status.idle":"2022-05-21T06:33:53.875486Z","shell.execute_reply.started":"2022-05-21T06:33:53.862273Z","shell.execute_reply":"2022-05-21T06:33:53.874421Z"},"trusted":true},"execution_count":null,"outputs":[]}]}