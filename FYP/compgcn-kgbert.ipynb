{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom transformers import BertTokenizerFast, BertModel\nimport torch_scatter\nimport inspect\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.cuda import amp\nimport ast\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-07T11:47:21.770117Z","iopub.execute_input":"2022-07-07T11:47:21.770538Z","iopub.status.idle":"2022-07-07T11:47:24.094540Z","shell.execute_reply.started":"2022-07-07T11:47:21.770464Z","shell.execute_reply":"2022-07-07T11:47:24.093762Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#!pip install torch-scatter","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.099663Z","iopub.execute_input":"2022-07-07T11:47:24.100074Z","iopub.status.idle":"2022-07-07T11:47:24.103907Z","shell.execute_reply.started":"2022-07-07T11:47:24.100037Z","shell.execute_reply":"2022-07-07T11:47:24.102967Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Create For basic learnable parameter, can be weighted matrix or basis vector","metadata":{}},{"cell_type":"code","source":"def get_param(shape):\n    param = nn.Parameter(torch.Tensor(*shape))\n    nn.init.xavier_normal_(param.data)\n    return param\ndef com_mult(a, b):\n    r1, i1 = a[:, 0], a[:, 1]\n    r2, i2 = b[:, 0], b[:, 1]\n    return torch.stack([r1 * r2 - i1 * i2, r1 * i2 + i1 * r2], dim = -1)\n\ndef conj(a):    \n    a[:, 1] = -a[:, 1]\n    return a\ndef ccorr(a, b):\n    return torch.irfft(com_mult(conj(torch.rfft(a, 1)), torch.rfft(b, 1)), 1, signal_sizes=(a.shape[-1],))","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.105484Z","iopub.execute_input":"2022-07-07T11:47:24.105882Z","iopub.status.idle":"2022-07-07T11:47:24.116268Z","shell.execute_reply.started":"2022-07-07T11:47:24.105844Z","shell.execute_reply":"2022-07-07T11:47:24.115210Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CompGcnBasis(nn.Module):\n    nodes_dim = 0\n    head_dim = 0\n    tail_dim = 1\n    def __init__(self, in_channels, out_channels, num_relations, num_basis_vector,act = torch.tanh,cache = True,dropout = 0.2):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_relations = num_relations\n        self.num_basis_vector = num_basis_vector\n        self.act = act\n        self.device = None\n        self.cache = cache\n        \n        #----------- Creating learnable basis vector , shape is (num_basis, feature_size(in channel))\n        self.basis_vector = get_param((num_basis_vector, in_channels))\n        # this weight matrix initialize the weight features for each relation(including inverse), shape is (2*num_relations, num_basis)\n        self.rel_weight = get_param((num_relations*2, self.num_basis_vector))\n        # this learnable weight matrix is for projection, that project each relation to the same dimension of node_dimension\n        self.weight_rel = get_param((in_channels,out_channels))\n        # add another embedding for loop\n        self.loop_rel = get_param((1,in_channels))\n        #----------- Creating three updated matrix, as three kind of relations updating, in, out, loop\n        # using for updating weight\n        self.w_in = get_param((in_channels,out_channels))\n        self.w_out = get_param((in_channels,out_channels))\n        self.w_loop = get_param((in_channels,out_channels))\n        \n        # define some helpful parameter\n        self.in_norm, self.out_norm = None, None\n        self.in_index, self.out_index = None, None\n        self.in_type, self.out_type = None, None\n        self.loop_index, self.loop_type =None, None\n        \n        self.drop = nn.Dropout(dropout)\n        self.bn = nn.BatchNorm1d(out_channels)\n    def relation_transform(self, entity_embedding, relation_embedding,type_):\n        '''\n        This function given entity embedding and relation embedding, in order return three types of \n        non-parameterized operations, which is subjection, corr, multiplication\n        '''\n        assert type_ in [\"mul\",\"sub\",\"corr\"], \"not implemented now\"\n        if type_ == \"mul\":\n            out = entity_embedding*relation_embedding\n        elif type_ == \"sub\":\n            out = entity_embedding - relation_embedding\n        else:\n            out = ccorr(entity_embedding,relation_embedding)\n        return out\n    \n    def normalization(self, edge_index, num_entity):\n        '''\n        As normal GCN, this function calculate the normalization adj matrix \n        '''\n        head, tail = edge_index\n        edge_weight = torch.ones_like(head).float()\n        degree = torch_scatter.scatter_add(edge_weight,head,dim_size=num_entity,dim = self.nodes_dim)\n        degree_inv = degree.pow(-0.5)\n        # if inf, in order to prevent nan in scatter function\n        degree_inv[degree_inv == float(\"inf\")] = 0\n        norm = degree_inv[head] * edge_weight * degree_inv[tail]\n        return norm\n    def scatter_function(self,type_, src, index, dim_size = None):\n        '''\n        This function given scatter_ type, which should me max, mean,or sum, given source array, given index array, given dimension size\n        '''\n        assert type_.lower() in [\"sum\",\"mean\",\"max\"]\n        return torch_scatter.scatter(src, index, dim=0,out=None,dim_size = dim_size, reduce= type_)\n    \n    def propogating_message(self, method, node_features,edge_index,edge_type, rel_embedding, edge_norm,mode,type_):\n        '''\n        This function done the basic aggregation\n        '''\n        assert method in [\"sum\", \"mean\", \"max\"]\n        assert mode in [\"in\",\"out\",\"loop\"]\n        size = node_features.shape[0]\n        coresponding_weight = getattr(self, 'w_{}'.format(mode))\n        #-------------- this index selection: given relation embedding and relation_basic representation, choose the inital basis vector part\n        relation_embedding = torch.index_select(rel_embedding,dim = 0, index = edge_type)\n        # ------------- using index of tail in edge index to represent head by relation\n        node_features = node_features[edge_index[1]]\n        out = self.relation_transform(node_features, relation_embedding,type_)\n        out = torch.matmul(out,coresponding_weight)\n        out = out if edge_norm is None else out * edge_norm.view(-1, 1)\n        out = self.scatter_function(method,out,edge_index[0],  size)\n        return out    \n    def forward(self, nodes_features, edge_index,edge_type):\n        '''\n        Forward propogate function:\n            Given input nodes_features, adj_matrix, relation_matrix\n        '''\n        with amp.autocast():\n            if self.device is None:\n                self.device = edge_index.device\n            # ----------- First done the basis part, which means represent each relation using a vector space defining previously\n            relation_embedding = torch.mm(self.rel_weight,self.basis_vector)\n            # ----------- add a self-loop dimension\n            relation_embedding = torch.cat([relation_embedding,self.loop_rel],dim = 0)\n            num_edges = edge_index.shape[1]//2\n            num_nodes = nodes_features.shape[self.nodes_dim]\n            if not self.cache or self.in_norm == None:\n                #---------------- in represent in_relation, out represent out_relation\n                self.in_index, self.out_index = edge_index[:,:num_edges], edge_index[:,num_edges:]\n                self.in_type, self.out_type = edge_type[:num_edges], edge_type[num_edges:]\n                # --------------- create self-loop part\n                self.loop_index = torch.stack([torch.arange(num_nodes), torch.arange(num_nodes)]).to(self.device)\n                self.loop_type = torch.full((num_nodes,), relation_embedding.shape[0]-1, dtype = torch.long).to(self.device)\n                # -------------- create normalization part\n                self.in_norm = self.normalization(self.in_index, num_nodes)\n                self.out_norm = self.normalization(self.out_index, num_nodes)\n            #print(self.in_norm.isinf().any())\n            in_res = self.propogating_message('sum',nodes_features,self.in_index,self.in_type, relation_embedding,self.in_norm,\"in\",\"sub\")\n            loop_res = self.propogating_message('sum',nodes_features,self.loop_index,self.loop_type, relation_embedding,None,\"loop\",\"sub\")\n            out_res = self.propogating_message('sum',nodes_features,self.out_index,self.out_type, relation_embedding,self.out_norm,\"out\",\"sub\")\n            # I don't know why but source code done it\n            out = self.drop(in_res)*(1/3) + self.drop(out_res)*(1/3) + loop_res*(1/3)\n            # update the relation embedding\n            out_2 = torch.matmul(relation_embedding,self.weight_rel)\n            return self.act(out),out_2","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.119808Z","iopub.execute_input":"2022-07-07T11:47:24.120248Z","iopub.status.idle":"2022-07-07T11:47:24.151270Z","shell.execute_reply.started":"2022-07-07T11:47:24.120208Z","shell.execute_reply":"2022-07-07T11:47:24.150600Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Test layer","metadata":{}},{"cell_type":"markdown","source":"## check some stats\n","metadata":{}},{"cell_type":"code","source":"class CompGcn_non_first_layer(nn.Module):\n    nodes_dim = 0\n    head_dim = 0\n    tail_dim = 1\n    def __init__(self, in_channels, out_channels, num_relations,act = torch.tanh,dropout = 0.2):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_relations = num_relations\n        self.act = act\n        self.device = None\n        \n        # this learnable weight matrix is for projection, that project each relation to the same dimension of node_dimension\n        self.weight_rel = get_param((in_channels,out_channels))\n        # add another embedding for loop\n        self.loop_rel = get_param((1,in_channels))\n        #----------- Creating three updated matrix, as three kind of relations updating, in, out, loop\n        # using for updating weight\n        self.w_in = get_param((in_channels,out_channels))\n        self.w_out = get_param((in_channels,out_channels))\n        self.w_loop = get_param((in_channels,out_channels))\n        self.drop = nn.Dropout(dropout)\n        self.bn = nn.BatchNorm1d(out_channels)\n    def relation_transform(self, entity_embedding, relation_embedding,type_):\n        '''\n        This function given entity embedding and relation embedding, in order return three types of \n        non-parameterized operations, which is subjection, corr, multiplication\n        '''\n        assert type_ in [\"mul\",\"sub\",\"corr\"], \"not implemented now\"\n        if type_ == \"mul\":\n            out = entity_embedding*relation_embedding\n        elif type_ == \"sub\":\n            out = entity_embedding - relation_embedding\n        else:\n            out = ccorr(entity_embedding,relation_embedding)\n        return out\n    \n    def normalization(self, edge_index, num_entity):\n        '''\n        As normal GCN, this function calculate the normalization adj matrix \n        '''\n        head, tail = edge_index\n        edge_weight = torch.ones_like(head).float()\n        degree = torch_scatter.scatter_add(edge_weight,head,dim_size=num_entity,dim = self.nodes_dim)\n        degree_inv = degree.pow(-0.5)\n        # if inf, in order to prevent nan in scatter function\n        degree_inv[degree_inv == float(\"inf\")] = 0\n        norm = degree_inv[head] * edge_weight * degree_inv[tail]\n        return norm\n    def scatter_function(self,type_, src, index, dim_size = None):\n        '''\n        This function given scatter_ type, which should me max, mean,or sum, given source array, given index array, given dimension size\n        '''\n        assert type_.lower() in [\"sum\",\"mean\",\"max\"]\n        return torch_scatter.scatter(src, index, dim=0,out=None,dim_size = dim_size, reduce= type_)\n    \n    def propogating_message(self, method, node_features,edge_index,edge_type, rel_embedding, edge_norm,mode,type_):\n        '''\n        This function done the basic aggregation\n        '''\n        assert method in [\"sum\", \"mean\", \"max\"]\n        assert mode in [\"in\",\"out\",\"loop\"]\n        size = node_features.shape[0]\n        coresponding_weight = getattr(self, 'w_{}'.format(mode))\n        #-------------- this index selection: given relation embedding and relation_basic representation, choose the inital basis vector part\n        relation_embedding = torch.index_select(rel_embedding,dim = 0, index = edge_type)\n        # ------------- using index of tail in edge index to represent head by relation\n        node_features = node_features[edge_index[1]]\n        out = self.relation_transform(node_features, relation_embedding,type_)\n        out = torch.matmul(out,coresponding_weight)\n        out = out if edge_norm is None else out * edge_norm.view(-1, 1)\n        out = self.scatter_function(method,out,edge_index[0],  size)\n        return out    \n    def forward(self, nodes_features, edge_index,edge_type,relation_embedding):\n        '''\n        Forward propogate function:\n            Given input nodes_features, adj_matrix, relation_matrix\n        '''\n        with amp.autocast():\n            if self.device is None:\n                self.device = edge_index.device\n            # ----------- add a self-loop dimension\n            relation_embedding = torch.cat([relation_embedding,self.loop_rel],dim = 0)\n            num_edges = edge_index.shape[1]//2\n            num_nodes = nodes_features.shape[self.nodes_dim]\n            #---------------- in represent in_relation, out represent out_relation\n            self.in_index, self.out_index = edge_index[:,:num_edges], edge_index[:,num_edges:]\n            self.in_type, self.out_type = edge_type[:num_edges], edge_type[num_edges:]\n            # --------------- create self-loop part\n            self.loop_index = torch.stack([torch.arange(num_nodes), torch.arange(num_nodes)]).to(self.device)\n            self.loop_type = torch.full((num_nodes,), relation_embedding.shape[0]-1, dtype = torch.long).to(self.device)\n            # -------------- create normalization part\n            self.in_norm = self.normalization(self.in_index, num_nodes)\n            self.out_norm = self.normalization(self.out_index, num_nodes)\n            #print(self.in_norm.isinf().any())\n            in_res = self.propogating_message('sum',nodes_features,self.in_index,self.in_type, relation_embedding,self.in_norm,\"in\",\"sub\")\n            loop_res = self.propogating_message('sum',nodes_features,self.loop_index,self.loop_type, relation_embedding,None,\"loop\",\"sub\")\n            out_res = self.propogating_message('sum',nodes_features,self.out_index,self.out_type, relation_embedding,self.out_norm,\"out\",\"sub\")\n            # I don't know why but source code done it\n            out = self.drop(in_res)*(1/3) + self.drop(out_res)*(1/3) + loop_res*(1/3)\n            # update the relation embedding\n            out_2 = torch.matmul(relation_embedding,self.weight_rel)\n            return self.act(out),out_2[:-1]# ignoring self loop inserted ","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.153576Z","iopub.execute_input":"2022-07-07T11:47:24.154294Z","iopub.status.idle":"2022-07-07T11:47:24.180531Z","shell.execute_reply.started":"2022-07-07T11:47:24.154256Z","shell.execute_reply":"2022-07-07T11:47:24.179824Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Test layer","metadata":{}},{"cell_type":"markdown","source":"## CompGCN total + score function","metadata":{}},{"cell_type":"code","source":"class CompGcn_total(nn.Module):\n    def __init__(self, channel_ls ,num_relation, num_basis_vector, edge_idx, edge_type,num_layers = 2, basis = False):\n        '''\n        Notice that in preprocessing, we assume that the node number will not be changed on the graph, only change relation.\n        input params:\n            1. channel_ls: a channel list containing all conv channel\n            2. num_relation, the number of relations_type for each graph(after preprocessing, should be the same for each graph)\n            3. num_basis_vector, the first layer basis of first graph.\n            4. edge_idx, adj matrix \n            5. edge_type, relation init\n            6. basis, whether need basis\n        '''\n        assert len(channel_ls) == num_layers + 1 , \"channel number should be layer numbers + 1 , got length \"+str(len(channel_ls))+\" with number of layers \"+str(num_layers)\n        super(CompGcn_total, self).__init__()\n        self.edge_idx = edge_idx\n        self.edge_type = edge_type\n        self.basis = basis\n        self.GCN_block = nn.Sequential()\n        for i in range(num_layers):\n            if basis and i == 0:\n                self.GCN_block.add_module(\"Basis_conv_layer\",  CompGcnBasis(in_channels = channel_ls[0], out_channels= channel_ls[1],\n                                                                            num_relations=num_relation,\n                                                                            num_basis_vector= num_basis_vector)) \n            else:\n                self.GCN_block.add_module(\"Conv_layer\"+str(i),CompGcn_non_first_layer(channel_ls[i], channel_ls[i+1], num_relation))\n    def forward(self, init_features = None, node_embd = None,rel_embd = None,device = None):\n        with amp.autocast():\n            for i, blk in enumerate(self.GCN_block):\n                if self.basis and i == 0:\n                    node_embd, rel_embd = blk(init_features, self.edge_idx.to(device), self.edge_type.to(device))\n                else:\n                    node_embd, rel_embd = blk(node_embd,self.edge_idx.to(device), self.edge_type.to(device), rel_embd)\n            return node_embd, rel_embd","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.181976Z","iopub.execute_input":"2022-07-07T11:47:24.182306Z","iopub.status.idle":"2022-07-07T11:47:24.196348Z","shell.execute_reply.started":"2022-07-07T11:47:24.182263Z","shell.execute_reply":"2022-07-07T11:47:24.195465Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Test layer","metadata":{}},{"cell_type":"code","source":"class CompGcn_with_temporal(nn.Module):\n    def __init__(self, conv_dim, num_relation, num_entity,node_dim, num_hiddens ,num_basis_vector, edge_idx, edge_type, model,num_class,time_stamp = 2,\n                 num_layers = 2,score_func=\"TransE\"):\n        '''\n        Notice that in preprocessing, we assume that the node number will not be changed on the graph, only change relation.\n        input params:\n            1. conv_dim, a list of tuple, [(channel 1, channel2, channel 3), (channel 3, channel 4, channel 5),...]\n            2. num_layer, number of CompGCN layer, now assume to 2 per graph\n            3. num_relation, the number of relations_type for each graph(after preprocessing, should be the same for each graph)\n            4. num_entity, number of entity, should be the same for each graph\n            5. node_dimension, dimension of nodes\n            6. num_basis_vector, the first layer basis of first graph.\n            7. edge_idx, a list of edge_idx\n            8. edge_type, a list of edge_type\n            9. num_class, classification number \n            10. time stamp: How many time steps \n        '''\n        assert len(conv_dim) == time_stamp, \"time stamp length should be the same as number of convloution dimension list!, got time stamp \"+str(time_stamp)+\" with conv_dim \"+str(len(conv_dim))\n        assert len(edge_idx) == len(edge_type) == len(conv_dim) == time_stamp, \"Number of KG mismatched with time stamp!\"\n        super(CompGcn_with_temporal,self).__init__()\n        self.model = model\n        self.num_relation = num_relation\n        self.node_features = get_param(shape= (num_entity,node_dim))\n        assert node_dim == conv_dim[0][0]\n        self.temporal_blk = nn.Sequential()\n        self.drop_bert = nn.Dropout(0.2)\n        for i in range(time_stamp):\n            if i == 0:\n                self.temporal_blk.add_module(\"Temporal block Basis\" , CompGcn_total(conv_dim[i], num_relation, num_basis_vector, \n                                                                                 edge_idx[i], edge_type[i], num_layers,True))\n            else:\n                self.temporal_blk.add_module(\"Temporal block\" + str(i), CompGcn_total(conv_dim[i], num_relation, num_basis_vector, \n                                                                                 edge_idx[i], edge_type[i], num_layers,False))\n        self.ln1 = nn.Linear(768 + conv_dim[-1][-1], num_class)\n        self.dropout_node = nn.Dropout(0.2)\n        self.dropout_rel = nn.Dropout(0.2)\n        self.ln1 = self.func_init(self.ln1)\n        self.score = score_func\n        # add GRU \n        self.W_xr = nn.Linear(conv_dim[0][-1],num_hiddens)\n        self.W_xz = nn.Linear(conv_dim[0][-1],num_hiddens)\n        self.W_xh = nn.Linear(conv_dim[0][-1],num_hiddens)\n        self.W_hr = nn.Linear(num_hiddens,num_hiddens, bias = True)\n        self.W_hz = nn.Linear(num_hiddens,num_hiddens, bias = True)\n        self.W_hh = nn.Linear(num_hiddens,num_hiddens, bias = True)\n        self.act_update = nn.Sigmoid()\n        self.act_hidden = nn.Tanh()\n        self.act_reset = nn.Sigmoid()\n    def func_init(self,m):\n        if type(m) == nn.Linear:\n            nn.init.xavier_uniform_(m.weight)\n        return m\n    def init_state(self, device):\n        return torch.zeros(self.num_relation * 2 + 1, conv_dim[1][0], device = device)\n    def forward(self, input_ids,segment_ids, attention_mask  ,head_index, tail_index, rel_index,state):\n        '''\n        Node index and rel index are corresponding information in a batch for bert part, we only care about the node, edge relation in a batch.\n        Since the embedding is tail - relation to head, the source will be tail, target will be head\n        '''\n        with amp.autocast():\n            device = self.node_features.device\n            bert_out = self.model(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = segment_ids)[\"pooler_output\"]\n            bert_out = self.drop_bert(bert_out)\n            for i, blk in enumerate(self.temporal_blk):\n                if i == 0:\n                    node_embd, rel_embd = blk(self.node_features, device = device)\n                else:\n                    node_embd, rel_embd = blk(node_embd = node_embd, rel_embd = rel_embd,device = device)\n                # with GRU\n                node_embd, rel_embd = self.dropout_node(node_embd), self.dropout_rel(rel_embd)\n                Z = self.act_update(self.W_xz(rel_embd) + self.W_hz(state))\n                R = self.act_reset(self.W_xr(rel_embd) + self.W_hr(state))\n                H_candidate = self.act_hidden(self.W_xh(rel_embd) + self.W_hh(R * state))\n                state = Z * state + (1 - Z) * H_candidate\n            # then choose corresponding index out:\n            # shape should be (len(index), hidden_out)\n            hidden_node_state = node_embd[tail_index,:]\n            hidden_rel_state  = rel_embd[rel_index,:]\n            hidden_target_state = node_embd[head_index,:]\n            head, rel, tail      = (\n                                        hidden_node_state, \n                                        hidden_rel_state, \n                                        hidden_target_state\n                                   )\n            score                = Score_func(head, rel, tail, func_type=self.score)\n            score                = score.forward_score()\n            score = torch.cat([score, bert_out], axis = 1)\n            score = self.ln1(score)\n        return score # hidden_node_state, hidden_rel_state, hidden_target_state","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.197919Z","iopub.execute_input":"2022-07-07T11:47:24.198321Z","iopub.status.idle":"2022-07-07T11:47:24.224845Z","shell.execute_reply.started":"2022-07-07T11:47:24.198269Z","shell.execute_reply":"2022-07-07T11:47:24.223914Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Score_func(nn.Module):\n    \"\"\"\n        Func:\n            Contain all the score functions we often meet. Now we finished ConvE, TransE, TransH, DisMult\n        \n        Args:\n            sub_emb: the head embedding (subject)\n            rel_emb: the relation embedding (relation)\n            obj_emb: the tail embedding (object)\n            kernel_size: a tuple. Only when the score function is ConvE, we need it to do the \n                        convolutional computation. i.e. kernel_size = (hight, width)\n            func_type: a string indicating the score function we wanna use. default to be \"TransE\"\n            conv_drop: a list containing floats, indicating the dropout rate we will use in the ConvE. \n                        If None, set to be all the same as \"dropout\" value. Default to all be the \n                        tuned parameter in compGCN paper.\n            conv_bias: whether to use bias. Default to be True\n            gamma: a float - margin hyperparameter. Only when we use TransE as our score function, we \n                    need it. Default to be 40.0, the tuned best parameter in compGCN.\n    \"\"\"\n    \n    def __init__(self, sub_emb, rel_emb, obj_emb, func_type=\"transE\", \n                 kernel_size = None, conv_drop=(0.2, 0.3, 0.2), \n                 conv_bias=True, gamma=40.0):\n        # we can't use self.__class__, because it may cause a recursive problem\n        super(Score_func, self).__init__()\n        \n        self.func_type = func_type.lower()\n        self.gamma     = gamma\n        self.sub_emb   = sub_emb\n        self.rel_emb   = rel_emb\n        self.obj_emb   = obj_emb\n        \n        if self.func_type == \"transh\":\n            self.relation_norm_embedding  = torch.nn.Embedding(num_embeddings=relation_num,\n                                                              embedding_dim=self.dimension)\n            self.relation_hyper_embedding = torch.nn.Embedding(num_embeddings=relation_num,\n                                                               embedding_dim=self.dimension)\n            self.entity_embedding         = torch.nn.Embedding(num_embeddings=entity_num,\n                                                               embedding_dim=self.dimension)\n        \n        if self.func_type == \"conve\":\n            assert not kernel_size is None  # to ensure that the kernel size is defined\n            \n            if not conv_drop:\n                self.hidden_drop = [dropout, dropout, dropout]\n            else:\n                l = len(hidden_drop)\n                assert l <= 3  # ensure the length of hidden_drop smaller equal to 3\n                if l == 1:\n                    self.conv_drop = [conv_drop[0], conv_drop[0], conv_drop[0]]\n                elif l == 2:\n                    self.conv_drop = [conv_drop[0], conv_drop[0], conv_drop[1]]\n                else:\n                    self.conv_drop = conv_drop\n                \n            \n            self.kernel_size    = kernel_size\n            self.bias           = conv_bias\n            \n            self.bn0            = torch.nn.BatchNorm2d(1)\n            self.bn1            = torch.nn.BatchNorm2d(self.out_channels)\n            self.bn2            = torch.nn.BatchNorm1d(self.kernel_size)\n\n            self.hidden_drop    = torch.nn.Dropout(self.conv_drop[0])\n            self.hidden_drop2   = torch.nn.Dropout(self.conv_drop[1])\n            self.feature_drop   = torch.nn.Dropout(self.conv_drop[2])\n            self.m_conv1        = torch.nn.Conv2d(1, out_channels=self.out_channels, \n                                                  kernel_size=(self.kernel_size, self.kernel_size), \n                                                  stride=1, padding=0, bias=self.bias)\n\n            flat_sz_h           = int(2*self.kernel_size[1]) - self.kernel_size + 1\n            flat_sz_w           = self.kernel_size[0] - self.kernel_size + 1\n            self.flat_sz        = flat_sz_h * flat_sz_w * self.out_channels\n            self.fc             = torch.nn.Linear(self.flat_sz, self.kernel_size)\n    \n    def concat(self, e1_embed, rel_embed):\n        e1_embed    = e1_embed. view(-1, 1, self.p.embed_dim)\n        rel_embed   = rel_embed.view(-1, 1, self.p.embed_dim)\n        stack_inp   = torch.cat([e1_embed, rel_embed], 1)\n        stack_inp   = torch.transpose(stack_inp, 2, 1).reshape((-1, 1, 2*self.p.k_w, self.p.k_h))\n        return stack_inp\n    \n    def projected(self, ent, norm):\n        norm = F.normalize(norm, p=2, dim=-1)\n        return ent - torch.sum(ent * norm, dim = 1, keepdim=True) * norm\n    \n    def forward_score(self):\n        with amp.autocast():\n            if   self.func_type == \"transe\":\n                x        = self.sub_emb + self.rel_emb - self.obj_emb\n            elif self.func_type == \"transh\":\n                head       = self.entity_embedding(self.sub_emb)\n                tail       = self.entity_embedding(self.obj_emb)\n                r_norm     = self.relation_norm_embedding(self.rel_emb)\n                r_hyper    = self.relation_hyper_embedding(self.rel_emb)\n                head_hyper = self.projected(head, r_norm)\n                tail_hyper = self.projected(tail, r_norm)\n                x          = torch.norm(head_hyper + r_hyper - tail_hyper, p=2, dim=2)\n            elif self.func_type == \"distmult\":\n                x        =   torch.mm(self.sub_emb + self.rel_emb, self.obj_emb.transpose(1, 0))\n                x        +=  self.bias.expand_as(x)\n            elif self.func_type == \"conve\":\n                stk_inp  = self.concat(sub_emb, rel_emb)\n                x        = self.bn0(stk_inp)\n                x        = self.m_conv1(x)\n                x        = self.bn1(x)\n                x        = F.relu(x)\n                x        = self.feature_drop(x)\n                x        = x.view(-1, self.flat_sz)\n                x        = self.fc(x)\n                x        = self.hidden_drop2(x)\n                x        = self.bn2(x)\n                x        = F.relu(x)\n\n                x = torch.mm(x, self.obj_emb.transpose(1,0))\n                x += self.bias.expand_as(x)\n            return x","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.226276Z","iopub.execute_input":"2022-07-07T11:47:24.226549Z","iopub.status.idle":"2022-07-07T11:47:24.253674Z","shell.execute_reply.started":"2022-07-07T11:47:24.226514Z","shell.execute_reply":"2022-07-07T11:47:24.252906Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# test = ('de/Franz_Tobisch',\n#  'Prabhas',\n#  'Igor_Strelbin',\n#  'Quentin_N._Burdick',\n#  'Bamir_Topi',\n#  'Federal_University_of_Amazonas',\n#  'Emmanuel_Baffour',\n#  'R.E.M.')","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.255404Z","iopub.execute_input":"2022-07-07T11:47:24.255932Z","iopub.status.idle":"2022-07-07T11:47:24.265672Z","shell.execute_reply.started":"2022-07-07T11:47:24.255881Z","shell.execute_reply":"2022-07-07T11:47:24.264866Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## 实现两种结合方法\n1. 不用子图，对整张图补零，然后再和语义信息结合之前选择导出对应的index，这样子开销会很大，不一定能够支持训练。\n2. 用子图，对子图补零，这样不会出现计算不了的问题，而且由于CompGCN 其可学习参数为 relation basis，relation embedding， node feature embd","metadata":{}},{"cell_type":"markdown","source":"## SubGraph implementation","metadata":{}},{"cell_type":"code","source":"# class CompGcn_subgraph(nn.Module):\n#     def __init__(self, num_entity,node_dim ):\n#         '''\n#         In a subgraph, we first need to choose the index, take all the index containing in it out:\n       \n#         '''\n#         super().__init__()\n#         self.node_features_whole = get_param(shape = (num_entity, node_dim))\n#     def select_index(self,input_):\n#         '''\n#         这个函数需要通过给定的input string，去找出整图中对应的edge_idx 和 edge_type(当然可以在外面做)， 然后根据对应的edge idx 的value 去原始的Node_features 里去找\n#         对应的子图的node feature，其维度应该为(num_nodes_sub, node_dim), 对于relation，则不需要改动，仍用整张图即可\n#         '''\n#         raise NotImplementedError\n#     def forward(self, input_):\n#         batch_node_feature, batch_edge_idx, batch_edge_type = self.select_index(*input_)\n#         return batch_node_feature","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.266906Z","iopub.execute_input":"2022-07-07T11:47:24.267295Z","iopub.status.idle":"2022-07-07T11:47:24.281153Z","shell.execute_reply.started":"2022-07-07T11:47:24.267260Z","shell.execute_reply":"2022-07-07T11:47:24.280407Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# class CompGcn_subgraph_temporal(CompGcn_subgraph):\n#     def __init__(self, conv_dim,num_relation,node_dim, num_basis_vector, edge_idx, edge_type):\n#         '''\n#         1. conv_dim: (in_channel, out_channel)\n#         2. num_relation: number of relation for whole graph\n#         4. node_dim: dimension for each node\n#         5. basis_vector\n#         6. a list of edge_idx for sub_graph\n#         6. a list of edge_type for sub_graph\n#         '''\n#         super().__init__(num_entity, node_dim)\n#         self.conv1 = CompGcn_total(conv_dim[0], conv_dim[1], num_relation, num_basis_vector, edge_idx[0],edge_type[0],True)\n#         self.conv2 = CompGcn_total(conv_dim[0], conv_dim[1], num_relation, num_basis_vector, edge_idx[1],edge_type[1],False)\n#         self.conv3 = CompGcn_total(conv_dim[0], conv_dim[1], num_relation, num_basis_vector, edge_idx[2],edge_type[2],False)\n#         self.RNN_nodes = nn.RNN(input_size = conv_dim[0], hidden_size = conv_dim[1])\n#         self.RNN_relembd = nn.RNN(input_size = conv_dim[0], hidden_size = conv_dim[1])\n#         self.dropout_node = nn.Dropout(0.2)\n#         self.dropout_rel = nn.Dropout(0.4)\n#     def forward2(self, input_string):\n#         batch_node_features, head_index, rel_index, tail_index =self.forward(input_string)\n#         node_embd1, rel_embd1 = self.conv1(batch_node_features) \n#         node_embd2, rel_embd2 = self.conv2(node_embeding = node_embd1, rel_embeding = rel_embd1)\n#         node_embd3, rel_embd3 = self.conv3(node_embeding = node_embd2, rel_embeding = rel_embd2)\n#         hidden_node_state = self.dropout_node(self.RNN_nodes(torch.cat([node_embd1[tail_index,:].unsqueeze(0), node_embd2[tail_index,:].unsqueeze(0), node_embd3[tail_index,:].unsqueeze(0)]))[0])\n#         hidden_rel_state = self.dropout_rel(self.RNN_relembd(torch.cat([rel_embd1[rel_index,:].unsqueeze(0), rel_embd2[rel_index,:].unsqueeze(0),rel_embd3[rel_index,:].unsqueeze(0)]))[0])\n#         hidden_node_state = self.dropout_node(self.RNN_nodes(torch.cat([node_embd1[head_index,:].unsqueeze(0), node_embd2[head_index,:].unsqueeze(0), node_embd3[head_index,:].unsqueeze(0)]))[0])\n#         return hidden_node_state[-1,:,:].squeeze(0), hidden_rel_state[-1,:,:].squeeze(0), hidden_target_state[-1,:,:].squeeze(0)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-07-07T11:47:24.282550Z","iopub.execute_input":"2022-07-07T11:47:24.282874Z","iopub.status.idle":"2022-07-07T11:47:24.291739Z","shell.execute_reply.started":"2022-07-07T11:47:24.282839Z","shell.execute_reply":"2022-07-07T11:47:24.290993Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Dataset and KG-Bert part:","metadata":{}},{"cell_type":"code","source":"def change_input(tokenizer, text1, text2=None, text3=None, labels = None,max_length=512):\n    '''\n    This function will change the given input from double to triple\n    '''\n    #do the basic tokenization without changing to index\n    tokens_1 = tokenizer.tokenize(text1)\n    if text2 is not None:\n        tokens_2 = tokenizer.tokenize(text2)\n    if text3 is not None:\n        tokens_3 = tokenizer.tokenize(text3)\n    #as shown in kg-bert, do the truncation\n    while True:\n        #do the trunctation \n        total_length = len(tokens_1)+len(tokens_2)+len(tokens_3)\n        if total_length<= max_length-4:\n            break\n        if len(tokens_1)>len(tokens_2) and len(tokens_1)>len(tokens_3):\n            tokens_1.pop()\n        elif len(tokens_2)>len(tokens_1) and len(tokens_2)>len(tokens_3):\n            tokens_2.pop()\n        elif len(tokens_3)>len(tokens_2) and len(tokens_3)>len(tokens_1):\n            tokens_3.pop()\n        else:\n            #else pop the token3(tail)\n            tokens_3.pop()\n    #segment encoding\n    final_token = [\"[CLS]\"]+tokens_1+[\"[SEP]\"]\n    #segment for first sentence\n    segment_ids = [0]*len(final_token)\n    if text2 is not None:\n        final_token+=tokens_2+[\"[SEP]\"]\n        segment_ids+=[1]*(len(tokens_2)+1)\n    if text3 is not None:\n        final_token+=tokens_3+[\"[SEP]\"]\n        segment_ids+=[0]*(len(tokens_3)+1)\n    #change it to the index\n    input_ids = tokenizer.convert_tokens_to_ids(final_token)\n    #for padding\n    padding = [0]*(max_length - len(input_ids))\n    #for attention mask\n    attention_mask = [1]*len(input_ids)\n    input_ids+=padding\n    attention_mask+= padding\n    segment_ids+=padding\n    assert len(input_ids) == max_length\n    assert len(attention_mask) == max_length\n    assert len(segment_ids) == max_length\n    return {\"input_ids\": input_ids,\n            \"segment_ids\": segment_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\":labels,\n    }","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.293964Z","iopub.execute_input":"2022-07-07T11:47:24.294239Z","iopub.status.idle":"2022-07-07T11:47:24.309251Z","shell.execute_reply.started":"2022-07-07T11:47:24.294203Z","shell.execute_reply":"2022-07-07T11:47:24.307388Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class language_Dataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        '''\n        df is dataframe given previously\n        '''\n        self.df = df\n        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        '''\n        This function will return the index\n        '''\n        dic = change_input(self.tokenizer,self.df.iloc[idx][\"head\"], self.df.iloc[idx][\"relation\"], self.df.iloc[idx][\"tail\"],self.df.iloc[idx][\"labels\"])\n        return torch.tensor(dic[\"input_ids\"]), torch.tensor(dic[\"segment_ids\"]), torch.tensor(dic[\"attention_mask\"]), torch.tensor(dic[\"labels\"]), self.df.iloc[idx][\"index_where\"]","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.312668Z","iopub.execute_input":"2022-07-07T11:47:24.313038Z","iopub.status.idle":"2022-07-07T11:47:24.323530Z","shell.execute_reply.started":"2022-07-07T11:47:24.313004Z","shell.execute_reply":"2022-07-07T11:47:24.322675Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def select_index(head, relation,tail,head2idx,rel2idx,edge_idx,edge_type):\n    '''\n    This function select the index given by correct index\n    '''\n    batch_head_tail_idx = [[head2idx[h], head2idx[t]] for h,t in zip(head, tail)]\n    rel_idx = [rel2idx[i] for i in relation]\n    ls = []\n    for i, j in zip(batch_head_tail_idx, rel_idx):\n        # column index\n        #print(np.argwhere(np.isin(edge_idx[0],i[0] )&np.isin(edge_idx[1], i[1])&np.isin(edge_type, j)))\n        ls.append(np.argwhere(np.isin(edge_idx[0],i[0] )&np.isin(edge_idx[1], i[1])&np.isin(edge_type, j))[0])          \n    idx = np.concatenate(ls)\n#     print(idx)\n    rel_value = edge_type[idx]\n    head_value, tail_value = edge_idx[:,idx]\n    assert len(head_value) == len(tail_value) == len(rel_value)\n    return head_value.tolist(), tail_value.tolist(), rel_value.tolist()\n# head_idx, tail_idx, rel_idx = select_index(head, relation,tail, head2idx, rel2idx,edge_idx_3, edge_type_2)\n# hidden_node, hidden_rel, hidden_target = tmp(input_ids, seg_ids, att_mask,head_idx,tail_idx, rel_idx)\n# hidden_node.shape, hidden_rel.shape, hidden_target.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.325085Z","iopub.execute_input":"2022-07-07T11:47:24.325379Z","iopub.status.idle":"2022-07-07T11:47:24.337395Z","shell.execute_reply.started":"2022-07-07T11:47:24.325344Z","shell.execute_reply":"2022-07-07T11:47:24.336580Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# def train_with_amp(net, train_set, criterion, optimizer, epochs,batch_size, scheduler, gradient_accumulate_step, max_grad_norm ,device):\n#     net.train()\n#     # instantiate a scalar object  \n#     print(\"train on \" + str(device))\n#     enable_amp = True if \"cuda\" in device.type else False\n#     scaler = amp.GradScaler(enabled= enable_amp)\n#     net.to(device)\n#     global_step = 0\n#     train_iter = torch.utils.data.DataLoader(train_set, batch_size = batch_size)\n#     for epoch in range(epochs):\n#         for idx, value in enumerate(train_iter):\n#             input_ids, seg_ids, att_mask, labels, index = value\n#             input_ids = input_ids.to(device)\n#             att_mask =att_mask.to(device)\n#             labels = labels.to(device)\n#             seg_ids = seg_ids.to(device)\n#             head_values = torch.tensor(index[0]).to(device)\n#             tail_values = torch.tensor(index[2]).to(device)\n#             rel_values = torch.tensor(index[1]).to(device)\n#             # when forward process, use amp\n#             with amp.autocast(enabled= enable_amp):\n#                 output = net(input_ids, seg_ids, att_mask,head_values,tail_values, rel_values)  \n#             loss = criterion(output, labels.view(-1,1).float())\n#             # prevent gradient to 0\n#             if gradient_accumulate_step > 1:\n#                 # 如果显存不足，通过 gradient_accumulate 来解决\n#                 loss = loss/gradient_accumulate_step\n            \n#             # 放大梯度，避免其消失\n#             scaler.scale(loss).backward()\n#             # do the gradient clip\n#             gradient_norm = nn.utils.clip_grad_norm_(net.parameters(),max_grad_norm)\n#             if (idx + 1) % gradient_accumulate_step == 0:\n#                 # 多少 step 更新一次梯度\n#                 # 通过 scaler.step 来unscale 回梯度值， 如果气结果不是infs 和Nans， 调用optimizer.step()来更新权重\n#                 # 否则忽略step调用， 保证权重不更新\n#                 scaler.step(optimizer)\n#                 scaler.update()\n#                 optimizer.zero_grad()\n#                 global_step += 1\n#                 scheduler.step()\n#             # 每100次计算 print 出一次loss\n#             if idx % 1000 == 0 or idx == len(train_iter) -1:\n#                 with torch.no_grad():\n#                     print(\"==============Epochs \"+ str(epoch) + \" ======================\")\n#                     print(\"loss: \" + str(loss) + \"; grad_norm: \" + str(gradient_norm))\n#                 torch.save({'epoch': epoch,\n#                 'model_state_dict': net.state_dict(),\n#                 'optimizer_state_dict': optimizer.state_dict(),\n#                 'loss': loss},\"./checkpoint.params\")\n#             print(\"successfully done one train\")","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:47:24.339477Z","iopub.execute_input":"2022-07-07T11:47:24.339724Z","iopub.status.idle":"2022-07-07T11:47:24.348036Z","shell.execute_reply.started":"2022-07-07T11:47:24.339697Z","shell.execute_reply":"2022-07-07T11:47:24.346135Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def try_gpu(i=0):\n    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n    if torch.cuda.device_count() >= i + 1:\n        return torch.device(f'cuda:{i}')\n    return torch.device('cpu')\n\n# In[ ]:\ndef train_with_amp(net, train_set, criterion, optimizer, epochs,batch_size, scheduler, gradient_accumulate_step, max_grad_norm , num_gpu):\n    net.train()   \n    \n    # instantiate a scalar object \n    ls          = []\n    device_ids  = [try_gpu(i) for i in range(num_gpu)]\n    device  = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(\"\\ntrain on %s\\n\"%str(device_ids))\n    enable_amp  = True if \"cuda\" in device_ids[0].type else False\n    scaler      = amp.GradScaler(enabled= enable_amp)\n    #net         = nn.DataParallel(net, device_ids = device_ids)\n    net.to(device)\n    train_iter  = torch.utils.data.DataLoader(train_set, batch_size = batch_size)\n    for epoch in range(epochs):\n        for idx, value in enumerate(train_iter):\n            ini_time    = time.time()\n            input_ids, seg_ids, att_mask, labels, index = value\n            input_ids   = input_ids.to(device_ids[0])\n            att_mask    = att_mask.to(device_ids[0])\n            labels      = labels.to(device_ids[0])\n            seg_ids     = seg_ids.to(device_ids[0])\n            head_values = torch.tensor(index[0]).to(device_ids[0])\n            tail_values = torch.tensor(index[2]).to(device_ids[0])\n            rel_values  = torch.tensor(index[1]).to(device_ids[0])\n            # when forward process, use amp\n            init_state = net.init_state(device)\n            with amp.autocast(enabled= enable_amp):\n                output  = net(input_ids, seg_ids, att_mask,head_values,tail_values, rel_values, init_state)  \n            loss        = criterion(output, labels.view(-1,1).float())\n            # prevent gradient to 0\n            if gradient_accumulate_step > 1:\n                # 如果显存不足，通过 gradient_accumulate 来解决\n                loss    = loss/gradient_accumulate_step\n            \n            # 放大梯度，避免其消失\n            scaler.scale(loss).mean().backward()\n            # do the gradient clip\n            gradient_norm = nn.utils.clip_grad_norm_(net.parameters(),max_grad_norm)\n            if (idx + 1) % gradient_accumulate_step == 0:\n                # 多少 step 更新一次梯度\n                # 通过 scaler.step 来unscale 回梯度值， 如果气结果不是infs 和Nans， 调用optimizer.step()来更新权重\n                # 否则忽略step调用， 保证权重不更新\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n                print(\"train 1 times\")\n            # 每1000次计算 print 出一次loss\n            if idx % 1000 == 0 or idx == len(train_iter) -1:\n                with torch.no_grad():\n                    print(\"==============Epochs \"+ str(epoch) + \" ======================\")\n                    print(\"loss: \" + str(loss) + \"; grad_norm: \" + str(gradient_norm))\n                ls.append(loss.item())\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': net.state_dict(),\n                    'param_groups': optimizer.state_dict()[\"param_groups\"],\n                    'loss': ls\n                },\"./checkpoint.params\")\n            with open(\"train_log\", \"a\") as f:\n                f.write(\"Epoch %s, Batch %s: %.4f sec\\n\"%(epoch, idx, time.time() - ini_time))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:48:57.831496Z","iopub.execute_input":"2022-07-07T11:48:57.831766Z","iopub.status.idle":"2022-07-07T11:48:57.849272Z","shell.execute_reply.started":"2022-07-07T11:48:57.831737Z","shell.execute_reply":"2022-07-07T11:48:57.848578Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    edge_idx_1 = torch.tensor(np.load(\"../input/3-graphs-info/edge_idx0.npz\")[\"arr_0\"])\n    edge_idx_2 = torch.tensor(np.load(\"../input/3-graphs-info/edge_idx1.npz\")[\"arr_0\"])\n    edge_idx_3 = torch.tensor(np.load(\"../input/3-graphs-info/edge_idx2.npz\")[\"arr_0\"])\n    num_nodes_0 = torch.tensor(np.load(\"../input/3-graphs-info/graph_0_num_nodes.npz\")[\"arr_0\"])\n    num_nodes_1 = torch.tensor(np.load(\"../input/3-graphs-info/graph_1_num_nodes.npz\")[\"arr_0\"])\n    num_nodes_2 = torch.tensor(np.load(\"../input/3-graphs-info/graph_2_num_nodes.npz\")[\"arr_0\"])\n    num_relation_0 = torch.tensor(np.load(\"../input/3-graphs-info/graph_0_num_edges.npz\")[\"arr_0\"])\n    num_relation_1 = torch.tensor(np.load(\"../input/3-graphs-info/graph_1_num_edges.npz\")[\"arr_0\"])\n    num_relation_2 = torch.tensor(np.load(\"../input/3-graphs-info/graph_2_num_edges.npz\")[\"arr_0\"])\n    edge_type_0 = torch.tensor(np.load(\"../input/3-graphs-info/edge_type0.npz\")[\"arr_0\"])\n    edge_type_1 = torch.tensor(np.load(\"../input/3-graphs-info/edge_type1.npz\")[\"arr_0\"])\n    edge_type_2 = torch.tensor(np.load(\"../input/3-graphs-info/edge_type2.npz\")[\"arr_0\"])\n    head2idx = np.load('../input/3-graphs-info/graph_2entity2index.npy', allow_pickle=True).item()\n    rel2idx =  np.load('../input/3-graphs-info/graph_2rel2index.npy', allow_pickle=True).item()\n    train = pd.read_csv(\"../input/train-valid-test-dataset/train.csv\").drop(\"Unnamed: 0\", axis = 1)\n    train[\"index_where\"] = train[\"index_where\"].apply(ast.literal_eval)\n    train_set = language_Dataset(train)\n    model = BertModel.from_pretrained('bert-base-uncased')\n    num_hiddens = 10\n    conv_dim, num_layer, node_dim, num_basis, edge_idx, edge_type = [[10, 20, num_hiddens], [10,20, num_hiddens], [10,20, num_hiddens]], 2, 10, 37, [edge_idx_1,edge_idx_2,edge_idx_3],[edge_type_0,edge_type_1,edge_type_2]\n    tmp = CompGcn_with_temporal(conv_dim,num_relation_2, num_nodes_2+1, node_dim, num_hiddens,num_basis,edge_idx, edge_type,model,1,3)\n    batch_size = 2\n    loss = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(tmp.parameters(), lr = 2e-4)\n    scheduler = get_cosine_schedule_with_warmup(optimizer= optimizer, num_warmup_steps = 0, num_training_steps= len(torch.utils.data.DataLoader(train_set, batch_size = batch_size)), num_cycles = 0.5)\n    train_with_amp(tmp, train_set, loss,optimizer,1,2, scheduler,1,1000,1)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:48:58.459747Z","iopub.execute_input":"2022-07-07T11:48:58.459993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(net, training_dataset, testing_dataset = None, lr = 0.01, loss_func=nn.CrossEntropyLoss ,\n#           num_epoches = 30, batch_size = 2, finetuning=False, plot=False):\n#     \"\"\"\n#         Func:\n#             To train the model\n        \n#         Args:\n#             net: the neuronal network\n#             training_dataset: a tensor form of dataset\n#             testing_dataset: a tensor form of dataset\n#             lr: learning rate\n#             loss_func: a callable loss function defined in torch.nn (for example, nn.CrossEntropyLoss())\n#             num_epoches: the number of epoches\n#             batch_size: the size of a batch\n#             finetuning: whether we need to initialize the weights. If true, pass a list \n#                         to indicate some specific layers, so that we can keep the weights fixed \n#                         within the specified layers (include the start and ending layer). For\n#                         example, [2, 4] means keep 2 to 4 layers fixed: [start_layer, end_layer].\n#                         We count layers begin from 0. Default to be False(initialize for every layer).\n#                         If it's True, we just use the pre-trained weights, and continue to train the \n#                         model.\n#     \"\"\"\n#     # to ensure we have a loss function\n#     assert loss_func != None, \"Must pass a loss function\"\n    \n#     device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n#     print(\"Training on \", device)\n    \n#     # move all the tensor to GPU\n#     def init_weights(m):\n#         if type(m) == nn.Linear or type(m) == nn.Conv2d:\n#             nn.init.xavier_uniform_(m.weight)#初始化\n#     if not finetuning and not type(finetuning) == list:\n#         net.apply(init_weights)#将其应用在每层网络\n#     elif type(finetuning) == list:\n#         # ensure the number is positive\n#         for i in range(len(finetuning)):\n#             if finetuning[i] < 0:\n#                 finetuning[i] += len(list(net.children()))\n#         if finetuning[0] > finetuning[1]:\n#             finetuning[0], finetuning[1] = finetuning[1], finetuning[0]\n#         count = 0\n#         para_optim = []\n#         for k in net.children():\n#             count += 1\n#             # finetuning layers should be changed properly\n#             if count > finetuning[1]:\n#                 for param in k.parameters():\n#                     para_optim.append(param)\n#             elif count < finetuning[0]:\n#                 for param in k.parameters():\n#                     para_optim.append(param)\n#             else:\n#                 for param in k.parameters():\n#                     param.requires_grad = False\n#     net.to(device)\n    \n#     training_iter = torch.utils.data.DataLoader(training_dataset, batch_size = batch_size, shuffle =True)\n#     if testing_dataset is not None:\n#         testing_iter = torch.utils.data.DataLoader(testing_dataset, shuffle = False, batch_size = batch_size)\n    \n#     # config the optimizer and the loss function\n#     # ------------------------------------------------   Change the optimizer here -------------------------------------------------- #\n#     optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr = lr)\n#     # ------------------------------------------------------------------------------------------------------------------------------- #\n    \n#     loss = loss_func\n#     if plot:\n#         animator = d2l.Animator(xlabel = \"epoch\", xlim=[1,num_epoches], legend=[\"train_loss\",\"train_Acc\", \"test_acc\"])\n#     num_batches = len(training_iter)\n\n#     # plot the loss dynamically\n#     train_loss = []\n#     train_acc = []\n#     net.train()    # turn on the training model\n    \n#     for epoch in range(num_epoches):\n#         # save a model every 2 epoches\n#         torch.save(model, \"./model_v01_epoch(%s).pt\"%(epoch+1))\n#         metric = d2l.Accumulator(3)\n#         # for every step\n#         for i,value in enumerate(training_iter):\n#             optimizer.zero_grad()\n#         # ------------------------------------------------   Change the training input here -------------------------------------------------- #\n#             input_ids, seg_ids, att_mask, labels, head, tail = value\n#             head_values, tail_values, rel_values = select_index(head, tail, head2idx, edge_idx_3, edge_type_2)\n#             input_ids = input_ids.to(device).long()\n#             att_mask =att_mask.to(device).long()\n#             labels = labels.to(device).long()\n#             seg_ids = seg_ids.to(device).long()\n#             output = net(input_ids, seg_ids, att_mask,head_values,tail_values, rel_values)\n#             print(output.shape)\n#         # ------------------------------------------------------------------------------------------------------------------------------------ #\n#             l = loss(output.float(), labels[0].long())\n#             l.backward()\n#             optimizer.step()\n#             with torch.no_grad():\n#                 metric.add(l * input_ids.shape[0], d2l.accuracy(output,labels), input_ids.shape[0])\n#                 train_loss.append(metric[0]/metric[2])\n#                 train_acc.append( metric[1]/metric[2])\n# #                 train_loss = metric[0]/metric[2]\n# #                 train_acc  = metric[1]/metric[2]\n# #                 if (i+1) % (num_batches//5) ==0 or i == num_batches-1:\n# #                     animator.add(epoch+(i+1)/num_batches, (train_loss,train_acc, None))\n#             #print(\"successfully train for period \",i)\n#             if plot and i % 1000 == 0:\n#                 with torch.no_grad():\n#                     dic = {\"epoch\":epoch,\"iteration\":i,\"optimizer\":optimizer.state_dict(),\"net\":net.state_dict()}\n#                     torch.save(dic,\"./model_params\")\n#                     plt.figure()\n#                     x_ = range(len(train_loss))\n#                     plt.plot(x_,train_loss,color=\"yellow\")\n#                     plt.plot(x_, train_acc,color = \"red\")\n#         if testing_dataset is not None:\n#             test_acc = evaluate_accuracy(net, graph_info, testing_iter)\n#             if plot:\n#                 animator.add(epoch+1,(None,None,test_acc))\n#         print(\"epoch %d, loss: %f\"%(epoch, train_loss[-1]))\n    \n#     return train_loss, train_acc","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:48:30.794753Z","iopub.status.idle":"2022-07-07T11:48:30.795288Z","shell.execute_reply.started":"2022-07-07T11:48:30.795005Z","shell.execute_reply":"2022-07-07T11:48:30.795034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from d2l import torch as d2l","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:48:30.796776Z","iopub.status.idle":"2022-07-07T11:48:30.797827Z","shell.execute_reply.started":"2022-07-07T11:48:30.797587Z","shell.execute_reply":"2022-07-07T11:48:30.797612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train(net,test_set)","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:48:30.799116Z","iopub.status.idle":"2022-07-07T11:48:30.799539Z","shell.execute_reply.started":"2022-07-07T11:48:30.799310Z","shell.execute_reply":"2022-07-07T11:48:30.799333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# conv_dim, num_layer, node_dim, num_basis, edge_idx, edge_type = [10, 20], 2, 10, 38, [edge_idx_1,edge_idx_2,edge_idx_3],[edge_type_0,edge_type_1,edge_type_2]\n# net = CompGcn_with_temporal(conv_dim,num_layer, num_relation_2+1, num_nodes_2+1, node_dim, num_basis,edge_idx, edge_type, model)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-07T11:48:30.800868Z","iopub.status.idle":"2022-07-07T11:48:30.801307Z","shell.execute_reply.started":"2022-07-07T11:48:30.801074Z","shell.execute_reply":"2022-07-07T11:48:30.801096Z"},"trusted":true},"execution_count":null,"outputs":[]}]}