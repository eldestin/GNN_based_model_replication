{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \n#from d2l import torch as d2l\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertModel\nfrom transformers import BertTokenizer\nimport os\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt \nos.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-30T07:45:23.001676Z","iopub.execute_input":"2022-04-30T07:45:23.002279Z","iopub.status.idle":"2022-04-30T07:45:25.017081Z","shell.execute_reply.started":"2022-04-30T07:45:23.002157Z","shell.execute_reply":"2022-04-30T07:45:25.015897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_data = pd.read_csv(\"../input/train-valid-test-dataset/train.csv\").drop(\"Unnamed: 0\", axis = 1)\n# train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-30T07:45:25.021551Z","iopub.execute_input":"2022-04-30T07:45:25.022073Z","iopub.status.idle":"2022-04-30T07:45:25.026014Z","shell.execute_reply.started":"2022-04-30T07:45:25.022029Z","shell.execute_reply":"2022-04-30T07:45:25.025222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Labeling","metadata":{}},{"cell_type":"code","source":"path = \"../input/yago-dataset/\"\ndef labeling_Dataset(path, times):\n    '''\n    This function is to label all the out-dated data from a given graph\n    path: location of tsv file \n    times: List: years given by user, [2012,2013] for example\n    '''\n    # read in the whole dataset\n    fact = pd.read_csv(path+\"yagoFacts.tsv\",sep=\"\\t\", header=None).drop(0,axis=0).drop(4,axis=1)\n    fact.columns = [\"ids\",\"head\",\"relation\",\"tail\"]\n    meta_fact = pd.read_csv(path+\"yagoMetaFacts.tsv\",sep = \"\\t\", header = None).drop(0)\n    meta_fact.columns = [\"ids1\",\"ids2\",\"relation\",\"date/others\",\"date\"]\n    # get the time info,use inner merge to get the existing result\n    time_info = meta_fact[~np.isnan(meta_fact[\"date\"])]\n    merge_info =pd.merge(time_info,fact,how = \"inner\",left_on=\"ids2\",right_on=\"ids\")\n    occur_since = merge_info[merge_info[\"relation_x\"] == \"<occursSince>\"]\n    occur_until = merge_info[merge_info[\"relation_x\"] == \"<occursUntil>\" ]\n    occur_since_care = occur_since[[\"ids\",\"date\"]].reset_index(drop = True)\n    care_until = occur_until[[\"ids\",\"head\",\"relation_y\",\"tail\",\"date\"]].reset_index(drop=True)\n    care_result = pd.merge(care_until,occur_since_care,how = \"left\", left_on=\"ids\",right_on=\"ids\")\n    care_result.columns = [\"ids\",\"head\",\"relation\",\"tail\",\"occur_until_date\",\"occur_since_date\"]\n    # for given time, labeling\n    # 先选出occur since的数据，代表出现在given time 之前\n    graphs = []\n    for time in tqdm(times):\n        fact_ = fact.copy()\n        # select triple smaller than time\n        graph = care_result[care_result[\"occur_since_date\"]<=time].copy()\n        #select ids greater than times\n        ids_later = care_result[care_result[\"occur_since_date\"]>time].ids\n        #判断occur until在不在given time 之前，如果在，则标为过时，否则为非过时\n        id_ = graph[graph[\"occur_until_date\"]<time].ids\n        # 先选出在之前的数据\n        fact_ = fact_[~fact_[\"ids\"].isin(ids_later)]\n        # 选出对应的ids，并与原先的fact对应\n        idx_out_dated = fact_[fact_[\"ids\"].isin(id_)].index\n        fact_[\"labels\"] = 1\n        fact_.loc[idx_out_dated,\"labels\"]=0\n        graphs.append(fact_)\n        del fact_\n    del fact\n    del meta_fact\n    gc.collect()\n    objective = graphs[-1]\n    final_graphs = []\n    for i in tqdm(graphs[:-1]):\n        tmp = objective.copy()\n        # 拿到在前面的图而不在最后一张图的index\n        idx = tmp[~tmp[\"ids\"].isin(i[\"ids\"])].index\n        #print(idx)\n        tmp.loc[idx, \"head\"] = \"<sp>\"\n        tmp.loc[idx, \"relation\"] = \"<sp>\"\n        tmp.loc[idx, \"tail\"] = \"<sp>\"\n        final_graphs.append(tmp.reset_index(drop=True))\n    final_graphs.append(objective.reset_index(drop=True))\n    return final_graphs\ngraphs = labeling_Dataset(path,[2010,2011,2012])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T07:45:25.028159Z","iopub.execute_input":"2022-04-30T07:45:25.02865Z","iopub.status.idle":"2022-04-30T07:46:37.83807Z","shell.execute_reply.started":"2022-04-30T07:45:25.028608Z","shell.execute_reply":"2022-04-30T07:46:37.837198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"graphs[-1]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T07:46:37.841054Z","iopub.execute_input":"2022-04-30T07:46:37.841401Z","iopub.status.idle":"2022-04-30T07:46:37.864593Z","shell.execute_reply.started":"2022-04-30T07:46:37.841357Z","shell.execute_reply":"2022-04-30T07:46:37.863924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Change to natural language","metadata":{}},{"cell_type":"code","source":"def preprocess(input_):\n    '''\n    Data is the given input dataframe from previous preprocessing\n    '''\n    data = input_.copy()\n    temp = data[data[\"tail\"].apply(lambda x: \"<\" in x)].copy()\n    for i in tqdm(temp.columns):\n        if i!=\"labels\":\n            data[i]=data[i].apply(lambda x: x.split(\"<\")[-1].split(\">\")[0])\n        else:\n            break\n    data.loc[:,\"tail\"] = data.loc[:,\"tail\"].apply(lambda x: x.split(\"^^\")[0])\n    del temp\n    gc.collect()\n    return data\ngraphs_ = [preprocess(i) for i in graphs]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T07:46:37.865532Z","iopub.execute_input":"2022-04-30T07:46:37.865752Z","iopub.status.idle":"2022-04-30T07:47:45.527913Z","shell.execute_reply.started":"2022-04-30T07:46:37.865726Z","shell.execute_reply":"2022-04-30T07:47:45.527244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Construct adj list","metadata":{}},{"cell_type":"code","source":"# only use one dictionary for relation and head \n# use the last graph\ndef build_dic(total_):\n    head_tail = pd.unique(pd.concat([total_[\"head\"],total_[\"tail\"]],ignore_index=True))\n    head_tail_index = range(1,len((head_tail))+1)\n    head2index = {}\n    for head,index in tqdm(zip(head_tail,head_tail_index)):\n        head2index[head] = index\n    head2index[\"sp\"] = 0\n    relation = pd.unique(total_[\"relation\"])\n    # create index of each relation\n    id_rel = range(1, len(relation) + 1)\n    rel2id = {rel:idx for idx, rel in zip(id_rel, relation)}\n    # add reverse relation part, which means head->tail is normal one, tail->head is reversed one\n    rel2id.update({rel + \"reverse\":idx + len(rel2id) for idx, rel in enumerate(relation)})\n    rel2id[\"sp\"] = 0\n    return head2index, rel2id\nhead2idx, rel2id = build_dic(graphs_[-1])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T07:47:45.530243Z","iopub.execute_input":"2022-04-30T07:47:45.530596Z","iopub.status.idle":"2022-04-30T07:47:53.362583Z","shell.execute_reply.started":"2022-04-30T07:47:45.530551Z","shell.execute_reply":"2022-04-30T07:47:53.36143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nearly cost 18 min for this cell\ndef build_adj_ls(total_, head2index, rel2id):\n    '''\n    input: total_, a DataFrame\n    '''\n    #创建索引和词语对应\n# --------------- original GAT preprocessing\n#     adj_ls = {}\n#    # create adj_list by using the index of each entity\n#    exist_entity = set()\n#     for i in tqdm(total_.iterrows()):\n#         if i[1][1] not in exist_entity:\n#             adj_ls[head2index[i[1][1]]] = [head2index[i[1][3]]]\n#             exist_entity.add(i[1][1])\n#         else:\n#             adj_ls[head2index[i[1][1]]].append(head2index[i[1][3]])\n\n\n    # --------------- Update preprocessing with relation embedding -----------\n    head_tail = pd.unique(pd.concat([total_[\"head\"],total_[\"tail\"]],ignore_index=True))\n    num_relation = len(rel2id) //2\n    ls = []\n    tmp_graph = total_.copy()\n    # temp dictionary for sampling \n    dic = {}\n    for i in tqdm(total_.iterrows()):\n        # here choose the correct value and add it into our last training graph\n        ls.append((head2index[i[1][1]], rel2id[i[1][2]], head2index[i[1][3]]))\n        if i[1][1] not in dic:\n            dic[i[1][1]] = i[1][0]\n        if i[1][3] not in dic:\n            dic[i[1][3]] = i[1][0]\n#     print(len(ls), total_.shape[0])\n#     print(len(ls) == total_.shape[0])\n    tmp_graph[\"index_where\"] = pd.Series(ls)\n    edge_idx, edge_type = [], []\n    # ------------- add original part\n    for sub, rel, obj in ls:\n        edge_idx.append((sub,obj))\n        edge_type.append(rel)\n    # ------------ add reverse part\n    for sub,rel,obj in ls:\n        edge_idx.append((obj,sub))\n        if rel != 0:\n            edge_type.append(rel + num_relation)\n        else:\n            edge_type.append(rel)\n    return np.array(edge_idx).T, np.array(edge_type), np.array(head_tail.shape[0]), np.array(num_relation), head2index, rel2id, tmp_graph, dic\n#edge_idx = [build_adj_ls(i,head2idx, rel2id) for i in graphs_]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T07:47:53.363923Z","iopub.execute_input":"2022-04-30T07:47:53.364209Z","iopub.status.idle":"2022-04-30T07:47:53.377451Z","shell.execute_reply.started":"2022-04-30T07:47:53.364179Z","shell.execute_reply":"2022-04-30T07:47:53.376635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_dataset(final_graph,dic,per):\n    '''\n    This function split the dataset into train and valid dataset.\n    Notice that we need to let all the entity be updated when training,\n    which means we need to cover all the entity in training set\n    params:\n        1. final_graph: latest graph\n        2. dic: init train_ids\n        3. per: percent\n    '''\n    init_set = pd.DataFrame({\"entity\":list(dic.keys()), \"ids\":list(dic.values())})\n    ids = pd.unique(init_set[\"ids\"])\n    init_train = final_graph[final_graph[\"ids\"].isin(ids)]\n    # others info \n    else_t = object_[-2][~object_[-2][\"ids\"].isin(ids)]\n    # split the rest and concat \n    train_else = else_t.sample(frac = per)\n    valid = else_t.sample(frac = 1-per)\n    return pd.concat([init_train, train_else]), valid","metadata":{"execution":{"iopub.status.busy":"2022-04-30T08:24:20.007358Z","iopub.execute_input":"2022-04-30T08:24:20.007693Z","iopub.status.idle":"2022-04-30T08:24:20.016151Z","shell.execute_reply.started":"2022-04-30T08:24:20.007661Z","shell.execute_reply":"2022-04-30T08:24:20.014953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_ = build_adj_ls(graphs_[-1], head2idx, rel2id)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T07:47:53.378427Z","iopub.execute_input":"2022-04-30T07:47:53.378645Z","iopub.status.idle":"2022-04-30T07:55:00.596575Z","shell.execute_reply.started":"2022-04-30T07:47:53.378619Z","shell.execute_reply":"2022-04-30T07:55:00.595305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, valid = split_dataset(object_[-2], object_[-1], 0.6)\ntrain.to_csv(\"./train.csv\")\nvalid.to_csv(\"./valid.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-30T08:27:14.493456Z","iopub.execute_input":"2022-04-30T08:27:14.494459Z","iopub.status.idle":"2022-04-30T08:28:19.371704Z","shell.execute_reply.started":"2022-04-30T08:27:14.494408Z","shell.execute_reply":"2022-04-30T08:28:19.37062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## (For Original GAT preprocessing)create edge index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # cost 24 second \n# def build_edge_index(adj_ls, num_nodes,add_self_edges = True):\n#     '''\n#     建立 edge 的矩阵，通过节点的index来表示edge\n#     '''\n#     head_ids, tail_ids = [],[]\n#     exist_edges = set()\n#     for head, tails in tqdm(adj_ls.items()):\n#         for tail in tails:\n#             if (head, tail) not in exist_edges:\n#                 head_ids.append(head)\n#                 tail_ids.append(tail)\n#                 exist_edges.add((head,tail))\n#     if add_self_edges:\n#         head_ids.extend(np.arange(num_nodes))\n#         tail_ids.extend(np.arange(num_nodes))\n    \n#     # shape为(2,num_edges)\n#     edge_idx = np.row_stack((head_ids, tail_ids))\n#     return edge_idx\n# # edge_idx = [build_edge_index(i[0],i[1]) for i in adj_list]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## save as npz","metadata":{}},{"cell_type":"code","source":"# for i,idx in enumerate(edge_idx):\n#     np.savez(\"edge_idx\"+str(i)+\".npz\",idx[0])\n#     np.savez(\"edge_type\"+str(i)+\".npz\",idx[1])\n#     np.savez(\"graph_\"+str(i)+\"_num_nodes.npz\",idx[2])\n#     np.savez(\"graph_\"+str(i)+\"_num_edges.npz\",idx[3])\n#     np.save(\"graph_\"+str(i)+\"entity2index.npy\", idx[4])\n#     np.save(\"graph_\"+str(i)+\"rel2index.npy\", idx[5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.chdir('/kaggle/working')\n# print(os.getcwd())\n# print(os.listdir(\"/kaggle/working\"))\n# from IPython.display import FileLink\n# FileLink('train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T09:28:46.599272Z","iopub.execute_input":"2022-04-10T09:28:46.599633Z","iopub.status.idle":"2022-04-10T09:28:46.609182Z","shell.execute_reply.started":"2022-04-10T09:28:46.599599Z","shell.execute_reply":"2022-04-10T09:28:46.60807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}