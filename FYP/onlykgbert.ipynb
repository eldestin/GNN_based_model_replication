{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom transformers import BertTokenizerFast, BertModel\nimport ast\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.cuda import amp\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-30T09:46:11.207477Z","iopub.execute_input":"2022-04-30T09:46:11.208082Z","iopub.status.idle":"2022-04-30T09:46:16.967694Z","shell.execute_reply.started":"2022-04-30T09:46:11.207911Z","shell.execute_reply":"2022-04-30T09:46:16.966784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def change_input(tokenizer, text1, text2=None, text3=None, labels = None,max_length=512):\n    '''\n    This function will change the given input from double to triple\n    '''\n    #do the basic tokenization without changing to index\n    tokens_1 = tokenizer.tokenize(text1)\n    if text2 is not None:\n        tokens_2 = tokenizer.tokenize(text2)\n    if text3 is not None:\n        tokens_3 = tokenizer.tokenize(text3)\n    #as shown in kg-bert, do the truncation\n    while True:\n        #do the trunctation \n        total_length = len(tokens_1)+len(tokens_2)+len(tokens_3)\n        if total_length<= max_length-4:\n            break\n        if len(tokens_1)>len(tokens_2) and len(tokens_1)>len(tokens_3):\n            tokens_1.pop()\n        elif len(tokens_2)>len(tokens_1) and len(tokens_2)>len(tokens_3):\n            tokens_2.pop()\n        elif len(tokens_3)>len(tokens_2) and len(tokens_3)>len(tokens_1):\n            tokens_3.pop()\n        else:\n            #else pop the token3(tail)\n            tokens_3.pop()\n    #segment encoding\n    final_token = [\"[CLS]\"]+tokens_1+[\"[SEP]\"]\n    #segment for first sentence\n    segment_ids = [0]*len(final_token)\n    if text2 is not None:\n        final_token+=tokens_2+[\"[SEP]\"]\n        segment_ids+=[1]*(len(tokens_2)+1)\n    if text3 is not None:\n        final_token+=tokens_3+[\"[SEP]\"]\n        segment_ids+=[0]*(len(tokens_3)+1)\n    #change it to the index\n    input_ids = tokenizer.convert_tokens_to_ids(final_token)\n    #for padding\n    padding = [0]*(max_length - len(input_ids))\n    #for attention mask\n    attention_mask = [1]*len(input_ids)\n    input_ids+=padding\n    attention_mask+= padding\n    segment_ids+=padding\n    assert len(input_ids) == max_length\n    assert len(attention_mask) == max_length\n    assert len(segment_ids) == max_length\n    return {\"input_ids\": input_ids,\n            \"segment_ids\": segment_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\":labels,\n    }","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:46:16.971665Z","iopub.execute_input":"2022-04-30T09:46:16.9723Z","iopub.status.idle":"2022-04-30T09:46:16.991985Z","shell.execute_reply.started":"2022-04-30T09:46:16.972256Z","shell.execute_reply":"2022-04-30T09:46:16.989844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class language_Dataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        '''\n        df is dataframe given previously\n        '''\n        self.df = df\n        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        '''\n        This function will return the index\n        '''\n        dic = change_input(self.tokenizer,self.df.iloc[idx][\"head\"], self.df.iloc[idx][\"relation\"], self.df.iloc[idx][\"tail\"],self.df.iloc[idx][\"labels\"])\n        return torch.tensor(dic[\"input_ids\"]), torch.tensor(dic[\"segment_ids\"]), torch.tensor(dic[\"attention_mask\"]), torch.tensor(dic[\"labels\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:46:16.994423Z","iopub.execute_input":"2022-04-30T09:46:16.995222Z","iopub.status.idle":"2022-04-30T09:46:17.010718Z","shell.execute_reply.started":"2022-04-30T09:46:16.995086Z","shell.execute_reply":"2022-04-30T09:46:17.00911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KGBERT(nn.Module):\n    def __init__(self,num_class,path = None):\n        '''\n        init function:\n            path: pretrained model from huggingface path, if not, download it from website\n        '''\n        super().__init__()\n        if path is not None:\n            self.model = BertModel.from_pretrained(path)\n        else:\n            self.model = BertModel.from_pretrained('bert-base-uncased')\n        self.ln1 = nn.Linear(768, num_class)\n    def forward(self, input_ids,segment_ids, attention_mask):\n        with amp.autocast():\n            bert_out = self.model(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = segment_ids)[\"pooler_output\"]\n            output = self.ln1(bert_out)\n            return output","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:46:17.015856Z","iopub.execute_input":"2022-04-30T09:46:17.016701Z","iopub.status.idle":"2022-04-30T09:46:17.0336Z","shell.execute_reply.started":"2022-04-30T09:46:17.016656Z","shell.execute_reply":"2022-04-30T09:46:17.032925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def try_gpu(i=0):\n    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n    if torch.cuda.device_count() >= i + 1:\n        return torch.device(f'cuda:{i}')\n    return torch.device('cpu')\ndef train_with_amp(net, train_set, criterion, optimizer, epochs,batch_size, scheduler, gradient_accumulate_step, max_grad_norm , num_gpu):\n    net.train()   \n    \n    # instantiate a scalar object \n    ls          = []\n    device_ids  = [try_gpu(i) for i in range(num_gpu)]\n    device  = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(\"\\ntrain on %s\\n\"%str(device_ids))\n    enable_amp  = True if \"cuda\" in device_ids[0].type else False\n    scaler      = amp.GradScaler(enabled= enable_amp)\n    net         = nn.DataParallel(net, device_ids = device_ids)\n    net.to(device)\n    train_iter  = torch.utils.data.DataLoader(train_set, batch_size = batch_size)\n    for epoch in range(epochs):\n        for idx, value in enumerate(train_iter):\n            ini_time    = time.time()\n            input_ids, seg_ids, att_mask, labels = value\n            input_ids   = input_ids.to(device_ids[0])\n            att_mask    = att_mask.to(device_ids[0])\n            labels      = labels.to(device_ids[0])\n            seg_ids     = seg_ids.to(device_ids[0])\n            # when forward process, use amp\n            with amp.autocast(enabled= enable_amp):\n                output  = net(input_ids, seg_ids, att_mask)  \n            loss        = criterion(output, labels.view(-1,1).float())\n            # prevent gradient to 0\n            if gradient_accumulate_step > 1:\n                # 如果显存不足，通过 gradient_accumulate 来解决\n                loss    = loss/gradient_accumulate_step\n            \n            # 放大梯度，避免其消失\n            scaler.scale(loss).mean().backward()\n            # do the gradient clip\n            gradient_norm = nn.utils.clip_grad_norm_(net.parameters(),max_grad_norm)\n            if (idx + 1) % gradient_accumulate_step == 0:\n                # 多少 step 更新一次梯度\n                # 通过 scaler.step 来unscale 回梯度值， 如果气结果不是infs 和Nans， 调用optimizer.step()来更新权重\n                # 否则忽略step调用， 保证权重不更新\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                scheduler.step()\n            # 每1000次计算 print 出一次loss\n            if idx % 1000 == 0 or idx == len(train_iter) -1:\n                with torch.no_grad():\n                    print(\"==============Epochs \"+ str(epoch) + \" ======================\")\n                    print(\"loss: \" + str(loss) + \"; grad_norm: \" + str(gradient_norm))\n                ls.append(loss.item())\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': net.state_dict(),\n                    'param_groups': optimizer.state_dict()[\"param_groups\"],\n                    'loss': ls\n                },\"./checkpoint.params\")\n            with open(\"train_log\", \"a\") as f:\n                f.write(\"Epoch %s, Batch %s: %.4f sec\\n\"%(epoch, idx, time.time() - ini_time))","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:49:17.305206Z","iopub.execute_input":"2022-04-30T09:49:17.30553Z","iopub.status.idle":"2022-04-30T09:49:17.325472Z","shell.execute_reply.started":"2022-04-30T09:49:17.305487Z","shell.execute_reply":"2022-04-30T09:49:17.324663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train = pd.read_csv(\"../input/train-valid-test-dataset/train.csv\").drop(\"Unnamed: 0\", axis = 1)\n    train[\"index_where\"] = train[\"index_where\"].apply(ast.literal_eval)\n    train_set = language_Dataset(train)\n    net = KGBERT(1)\n    loss = nn.BCEWithLogitsLoss()\n    batch_size = 2\n    lr = 2e-6\n    num_gpu = 1\n    optimizer = torch.optim.AdamW(net.parameters(), lr = lr)\n    scheduler = get_cosine_schedule_with_warmup(optimizer= optimizer, num_warmup_steps = 0, \n                                                num_training_steps= len(torch.utils.data.DataLoader(train_set, batch_size = batch_size)), num_cycles = 0.5)\n    train_with_amp(net, train_set,loss,optimizer,3,batch_size,scheduler,1,1000,num_gpu)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T09:54:32.21368Z","iopub.execute_input":"2022-04-30T09:54:32.216213Z","iopub.status.idle":"2022-04-30T09:54:37.612087Z","shell.execute_reply.started":"2022-04-30T09:54:32.216161Z","shell.execute_reply":"2022-04-30T09:54:37.610616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}