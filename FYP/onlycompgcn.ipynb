{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch-scatter","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:47:07.586605Z","iopub.execute_input":"2022-04-30T11:47:07.587212Z","iopub.status.idle":"2022-04-30T11:55:30.991958Z","shell.execute_reply.started":"2022-04-30T11:47:07.587115Z","shell.execute_reply":"2022-04-30T11:55:30.991103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom transformers import BertTokenizerFast, BertModel\nimport torch_scatter\nimport inspect\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.cuda import amp\nimport ast","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-30T11:55:30.995562Z","iopub.execute_input":"2022-04-30T11:55:30.996274Z","iopub.status.idle":"2022-04-30T11:55:36.173099Z","shell.execute_reply.started":"2022-04-30T11:55:30.996225Z","shell.execute_reply":"2022-04-30T11:55:36.172363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_param(shape):\n    param = nn.Parameter(torch.Tensor(*shape))\n    nn.init.xavier_normal_(param.data)\n    return param\ndef com_mult(a, b):\n    r1, i1 = a[:, 0], a[:, 1]\n    r2, i2 = b[:, 0], b[:, 1]\n    return torch.stack([r1 * r2 - i1 * i2, r1 * i2 + i1 * r2], dim = -1)\n\ndef conj(a):    \n    a[:, 1] = -a[:, 1]\n    return a\ndef ccorr(a, b):\n    return torch.irfft(com_mult(conj(torch.rfft(a, 1)), torch.rfft(b, 1)), 1, signal_sizes=(a.shape[-1],))\n\nclass CompGcnBasis(nn.Module):\n    nodes_dim = 0\n    head_dim = 0\n    tail_dim = 1\n    def __init__(self, in_channels, out_channels, num_relations, num_basis_vector,act = torch.tanh,cache = True,dropout = 0.2):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_relations = num_relations\n        self.num_basis_vector = num_basis_vector\n        self.act = act\n        self.device = None\n        self.cache = cache\n        \n        #----------- Creating learnable basis vector , shape is (num_basis, feature_size(in channel))\n        self.basis_vector = get_param((num_basis_vector, in_channels))\n        # this weight matrix initialize the weight features for each relation(including inverse), shape is (2*num_relations, num_basis)\n        self.rel_weight = get_param((num_relations*2, self.num_basis_vector))\n        # this learnable weight matrix is for projection, that project each relation to the same dimension of node_dimension\n        self.weight_rel = get_param((in_channels,out_channels))\n        # add another embedding for loop\n        self.loop_rel = get_param((1,in_channels))\n        #----------- Creating three updated matrix, as three kind of relations updating, in, out, loop\n        # using for updating weight\n        self.w_in = get_param((in_channels,out_channels))\n        self.w_out = get_param((in_channels,out_channels))\n        self.w_loop = get_param((in_channels,out_channels))\n        \n        # define some helpful parameter\n        self.in_norm, self.out_norm = None, None\n        self.in_index, self.out_index = None, None\n        self.in_type, self.out_type = None, None\n        self.loop_index, self.loop_type =None, None\n        \n        self.drop = nn.Dropout(dropout)\n        self.bn = nn.BatchNorm1d(out_channels)\n    def relation_transform(self, entity_embedding, relation_embedding,type_):\n        '''\n        This function given entity embedding and relation embedding, in order return three types of \n        non-parameterized operations, which is subjection, corr, multiplication\n        '''\n        assert type_ in [\"mul\",\"sub\",\"corr\"], \"not implemented now\"\n        if type_ == \"mul\":\n            out = entity_embedding*relation_embedding\n        elif type_ == \"sub\":\n            out = entity_embedding - relation_embedding\n        else:\n            out = ccorr(entity_embedding,relation_embedding)\n        return out\n    \n    def normalization(self, edge_index, num_entity):\n        '''\n        As normal GCN, this function calculate the normalization adj matrix \n        '''\n        head, tail = edge_index\n        edge_weight = torch.ones_like(head).float()\n        degree = torch_scatter.scatter_add(edge_weight,head,dim_size=num_entity,dim = self.nodes_dim)\n        degree_inv = degree.pow(-0.5)\n        # if inf, in order to prevent nan in scatter function\n        degree_inv[degree_inv == float(\"inf\")] = 0\n        norm = degree_inv[head] * edge_weight * degree_inv[tail]\n        return norm\n    def scatter_function(self,type_, src, index, dim_size = None):\n        '''\n        This function given scatter_ type, which should me max, mean,or sum, given source array, given index array, given dimension size\n        '''\n        assert type_.lower() in [\"sum\",\"mean\",\"max\"]\n        return torch_scatter.scatter(src, index, dim=0,out=None,dim_size = dim_size, reduce= type_)\n    \n    def propogating_message(self, method, node_features,edge_index,edge_type, rel_embedding, edge_norm,mode,type_):\n        '''\n        This function done the basic aggregation\n        '''\n        assert method in [\"sum\", \"mean\", \"max\"]\n        assert mode in [\"in\",\"out\",\"loop\"]\n        size = node_features.shape[0]\n        coresponding_weight = getattr(self, 'w_{}'.format(mode))\n        #-------------- this index selection: given relation embedding and relation_basic representation, choose the inital basis vector part\n        relation_embedding = torch.index_select(rel_embedding,dim = 0, index = edge_type)\n        # ------------- using index of tail in edge index to represent head by relation\n        node_features = node_features[edge_index[1]]\n        out = self.relation_transform(node_features, relation_embedding,type_)\n        out = torch.matmul(out,coresponding_weight)\n        out = out if edge_norm is None else out * edge_norm.view(-1, 1)\n        out = self.scatter_function(method,out,edge_index[0],  size)\n        return out    \n    def forward(self, nodes_features, edge_index,edge_type):\n        '''\n        Forward propogate function:\n            Given input nodes_features, adj_matrix, relation_matrix\n        '''\n        with amp.autocast():\n            if self.device is None:\n                self.device = edge_index.device\n            # ----------- First done the basis part, which means represent each relation using a vector space defining previously\n            relation_embedding = torch.mm(self.rel_weight,self.basis_vector)\n            # ----------- add a self-loop dimension\n            relation_embedding = torch.cat([relation_embedding,self.loop_rel],dim = 0)\n            num_edges = edge_index.shape[1]//2\n            num_nodes = nodes_features.shape[self.nodes_dim]\n            if not self.cache or self.in_norm == None:\n                #---------------- in represent in_relation, out represent out_relation\n                self.in_index, self.out_index = edge_index[:,:num_edges], edge_index[:,num_edges:]\n                self.in_type, self.out_type = edge_type[:num_edges], edge_type[num_edges:]\n                # --------------- create self-loop part\n                self.loop_index = torch.stack([torch.arange(num_nodes), torch.arange(num_nodes)]).to(self.device)\n                self.loop_type = torch.full((num_nodes,), relation_embedding.shape[0]-1, dtype = torch.long).to(self.device)\n                # -------------- create normalization part\n                self.in_norm = self.normalization(self.in_index, num_nodes)\n                self.out_norm = self.normalization(self.out_index, num_nodes)\n            #print(self.in_norm.isinf().any())\n            in_res = self.propogating_message('sum',nodes_features,self.in_index,self.in_type, relation_embedding,self.in_norm,\"in\",\"sub\")\n            loop_res = self.propogating_message('sum',nodes_features,self.loop_index,self.loop_type, relation_embedding,None,\"loop\",\"sub\")\n            out_res = self.propogating_message('sum',nodes_features,self.out_index,self.out_type, relation_embedding,self.out_norm,\"out\",\"sub\")\n            # I don't know why but source code done it\n            out = self.drop(in_res)*(1/3) + self.drop(out_res)*(1/3) + loop_res*(1/3)\n            # update the relation embedding\n            out_2 = torch.matmul(relation_embedding,self.weight_rel)\n            return self.act(out),out_2","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:55:36.174686Z","iopub.execute_input":"2022-04-30T11:55:36.174942Z","iopub.status.idle":"2022-04-30T11:55:36.21042Z","shell.execute_reply.started":"2022-04-30T11:55:36.174908Z","shell.execute_reply":"2022-04-30T11:55:36.209627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CompGcn_non_first_layer(nn.Module):\n    nodes_dim = 0\n    head_dim = 0\n    tail_dim = 1\n    def __init__(self, in_channels, out_channels, num_relations,act = torch.tanh,dropout = 0.2):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_relations = num_relations\n        self.act = act\n        self.device = None\n        \n        # this learnable weight matrix is for projection, that project each relation to the same dimension of node_dimension\n        self.weight_rel = get_param((in_channels,out_channels))\n        # add another embedding for loop\n        self.loop_rel = get_param((1,in_channels))\n        #----------- Creating three updated matrix, as three kind of relations updating, in, out, loop\n        # using for updating weight\n        self.w_in = get_param((in_channels,out_channels))\n        self.w_out = get_param((in_channels,out_channels))\n        self.w_loop = get_param((in_channels,out_channels))\n        self.drop = nn.Dropout(dropout)\n        self.bn = nn.BatchNorm1d(out_channels)\n    def relation_transform(self, entity_embedding, relation_embedding,type_):\n        '''\n        This function given entity embedding and relation embedding, in order return three types of \n        non-parameterized operations, which is subjection, corr, multiplication\n        '''\n        assert type_ in [\"mul\",\"sub\",\"corr\"], \"not implemented now\"\n        if type_ == \"mul\":\n            out = entity_embedding*relation_embedding\n        elif type_ == \"sub\":\n            out = entity_embedding - relation_embedding\n        else:\n            out = ccorr(entity_embedding,relation_embedding)\n        return out\n    \n    def normalization(self, edge_index, num_entity):\n        '''\n        As normal GCN, this function calculate the normalization adj matrix \n        '''\n        head, tail = edge_index\n        edge_weight = torch.ones_like(head).float()\n        degree = torch_scatter.scatter_add(edge_weight,head,dim_size=num_entity,dim = self.nodes_dim)\n        degree_inv = degree.pow(-0.5)\n        # if inf, in order to prevent nan in scatter function\n        degree_inv[degree_inv == float(\"inf\")] = 0\n        norm = degree_inv[head] * edge_weight * degree_inv[tail]\n        return norm\n    def scatter_function(self,type_, src, index, dim_size = None):\n        '''\n        This function given scatter_ type, which should me max, mean,or sum, given source array, given index array, given dimension size\n        '''\n        assert type_.lower() in [\"sum\",\"mean\",\"max\"]\n        return torch_scatter.scatter(src, index, dim=0,out=None,dim_size = dim_size, reduce= type_)\n    \n    def propogating_message(self, method, node_features,edge_index,edge_type, rel_embedding, edge_norm,mode,type_):\n        '''\n        This function done the basic aggregation\n        '''\n        assert method in [\"sum\", \"mean\", \"max\"]\n        assert mode in [\"in\",\"out\",\"loop\"]\n        size = node_features.shape[0]\n        coresponding_weight = getattr(self, 'w_{}'.format(mode))\n        #-------------- this index selection: given relation embedding and relation_basic representation, choose the inital basis vector part\n        relation_embedding = torch.index_select(rel_embedding,dim = 0, index = edge_type)\n        # ------------- using index of tail in edge index to represent head by relation\n        node_features = node_features[edge_index[1]]\n        out = self.relation_transform(node_features, relation_embedding,type_)\n        out = torch.matmul(out,coresponding_weight)\n        out = out if edge_norm is None else out * edge_norm.view(-1, 1)\n        out = self.scatter_function(method,out,edge_index[0],  size)\n        return out    \n    def forward(self, nodes_features, edge_index,edge_type,relation_embedding):\n        '''\n        Forward propogate function:\n            Given input nodes_features, adj_matrix, relation_matrix\n        '''\n        with amp.autocast():\n            if self.device is None:\n                self.device = edge_index.device\n            # ----------- add a self-loop dimension\n            relation_embedding = torch.cat([relation_embedding,self.loop_rel],dim = 0)\n            num_edges = edge_index.shape[1]//2\n            num_nodes = nodes_features.shape[self.nodes_dim]\n            #---------------- in represent in_relation, out represent out_relation\n            self.in_index, self.out_index = edge_index[:,:num_edges], edge_index[:,num_edges:]\n            self.in_type, self.out_type = edge_type[:num_edges], edge_type[num_edges:]\n            # --------------- create self-loop part\n            self.loop_index = torch.stack([torch.arange(num_nodes), torch.arange(num_nodes)]).to(self.device)\n            self.loop_type = torch.full((num_nodes,), relation_embedding.shape[0]-1, dtype = torch.long).to(self.device)\n            # -------------- create normalization part\n            self.in_norm = self.normalization(self.in_index, num_nodes)\n            self.out_norm = self.normalization(self.out_index, num_nodes)\n            #print(self.in_norm.isinf().any())\n            in_res = self.propogating_message('sum',nodes_features,self.in_index,self.in_type, relation_embedding,self.in_norm,\"in\",\"sub\")\n            loop_res = self.propogating_message('sum',nodes_features,self.loop_index,self.loop_type, relation_embedding,None,\"loop\",\"sub\")\n            out_res = self.propogating_message('sum',nodes_features,self.out_index,self.out_type, relation_embedding,self.out_norm,\"out\",\"sub\")\n            # I don't know why but source code done it\n            out = self.drop(in_res)*(1/3) + self.drop(out_res)*(1/3) + loop_res*(1/3)\n            # update the relation embedding\n            out_2 = torch.matmul(relation_embedding,self.weight_rel)\n            return self.act(out),out_2[:-1]# ignoring self loop inserted ","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:55:36.212564Z","iopub.execute_input":"2022-04-30T11:55:36.213015Z","iopub.status.idle":"2022-04-30T11:55:36.23844Z","shell.execute_reply.started":"2022-04-30T11:55:36.212974Z","shell.execute_reply":"2022-04-30T11:55:36.237561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CompGcn_total(nn.Module):\n    def __init__(self, in_channel, out_channel,num_relation, num_basis_vector, edge_idx, edge_type, basis = False):\n        '''\n        Notice that in preprocessing, we assume that the node number will not be changed on the graph, only change relation.\n        input params:\n            1. conv_dim, a list/tuple that include the convolution in_channel, out_channel, like [[in_1, out_1], [out_1, out_2]]. Assume that each graph has the same dim\n            2. num_relation, the number of relations_type for each graph(after preprocessing, should be the same for each graph)\n            3. num_basis_vector, the first layer basis of first graph.\n            4. edge_idx, adj matrix \n            5. edge_type, relation init\n            6. basis, whether need basis\n        '''\n        super().__init__()\n#         self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        self.edge_idx = edge_idx\n        self.edge_type = edge_type\n        self.basis = basis\n        if basis:\n            self.conv1 = CompGcnBasis(in_channels = in_channel, out_channels= out_channel, num_relations=num_relation, num_basis_vector= num_basis_vector)\n            self.conv2 = CompGcn_non_first_layer(out_channel, out_channel, num_relation)\n        else:\n            self.conv1 = CompGcn_non_first_layer(in_channel, out_channel, num_relation)\n            self.conv2 = CompGcn_non_first_layer(out_channel, out_channel, num_relation)\n    def forward(self, init_features = None, node_embeding = None,rel_embeding = None,device = None):\n        with amp.autocast():\n            if self.basis:\n                node_embd, rel_embd = self.conv1(init_features, self.edge_idx.to(device), self.edge_type.to(device))\n                node_embd, rel_embd = self.conv2(node_embd,self.edge_idx.to(device), self.edge_type.to(device), rel_embd)\n            else:\n                node_embd, rel_embd = self.conv1(node_embeding,self.edge_idx.to(device), self.edge_type.to(device), rel_embeding)\n                node_embd, rel_embd = self.conv2(node_embd,self.edge_idx.to(device), self.edge_type.to(device), rel_embd)\n            #print(node_embd.shape, rel_embd.shape)\n            return node_embd, rel_embd","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:55:36.240952Z","iopub.execute_input":"2022-04-30T11:55:36.241216Z","iopub.status.idle":"2022-04-30T11:55:36.254543Z","shell.execute_reply.started":"2022-04-30T11:55:36.241178Z","shell.execute_reply":"2022-04-30T11:55:36.253698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Score_func(nn.Module):\n    \"\"\"\n        Func:\n            Contain all the score functions we often meet. Now we finished ConvE, TransE, TransH, DisMult\n        \n        Args:\n            sub_emb: the head embedding (subject)\n            rel_emb: the relation embedding (relation)\n            obj_emb: the tail embedding (object)\n            kernel_size: a tuple. Only when the score function is ConvE, we need it to do the \n                        convolutional computation. i.e. kernel_size = (hight, width)\n            func_type: a string indicating the score function we wanna use. default to be \"TransE\"\n            conv_drop: a list containing floats, indicating the dropout rate we will use in the ConvE. \n                        If None, set to be all the same as \"dropout\" value. Default to all be the \n                        tuned parameter in compGCN paper.\n            conv_bias: whether to use bias. Default to be True\n            gamma: a float - margin hyperparameter. Only when we use TransE as our score function, we \n                    need it. Default to be 40.0, the tuned best parameter in compGCN.\n    \"\"\"\n    \n    def __init__(self, sub_emb, rel_emb, obj_emb, func_type=\"transE\", \n                 kernel_size = None, conv_drop=(0.2, 0.3, 0.2), \n                 conv_bias=True, gamma=40.0):\n        # we can't use self.__class__, because it may cause a recursive problem\n        super(Score_func, self).__init__()\n        \n        self.func_type = func_type.lower()\n        self.gamma     = gamma\n        self.sub_emb   = sub_emb\n        self.rel_emb   = rel_emb\n        self.obj_emb   = obj_emb\n        \n        if self.func_type == \"transh\":\n            self.relation_norm_embedding  = torch.nn.Embedding(num_embeddings=relation_num,\n                                                              embedding_dim=self.dimension)\n            self.relation_hyper_embedding = torch.nn.Embedding(num_embeddings=relation_num,\n                                                               embedding_dim=self.dimension)\n            self.entity_embedding         = torch.nn.Embedding(num_embeddings=entity_num,\n                                                               embedding_dim=self.dimension)\n        \n        if self.func_type == \"conve\":\n            assert not kernel_size is None  # to ensure that the kernel size is defined\n            \n            if not conv_drop:\n                self.hidden_drop = [dropout, dropout, dropout]\n            else:\n                l = len(hidden_drop)\n                assert l <= 3  # ensure the length of hidden_drop smaller equal to 3\n                if l == 1:\n                    self.conv_drop = [conv_drop[0], conv_drop[0], conv_drop[0]]\n                elif l == 2:\n                    self.conv_drop = [conv_drop[0], conv_drop[0], conv_drop[1]]\n                else:\n                    self.conv_drop = conv_drop\n                \n            \n            self.kernel_size    = kernel_size\n            self.bias           = conv_bias\n            \n            self.bn0            = torch.nn.BatchNorm2d(1)\n            self.bn1            = torch.nn.BatchNorm2d(self.out_channels)\n            self.bn2            = torch.nn.BatchNorm1d(self.kernel_size)\n\n            self.hidden_drop    = torch.nn.Dropout(self.conv_drop[0])\n            self.hidden_drop2   = torch.nn.Dropout(self.conv_drop[1])\n            self.feature_drop   = torch.nn.Dropout(self.conv_drop[2])\n            self.m_conv1        = torch.nn.Conv2d(1, out_channels=self.out_channels, \n                                                  kernel_size=(self.kernel_size, self.kernel_size), \n                                                  stride=1, padding=0, bias=self.bias)\n\n            flat_sz_h           = int(2*self.kernel_size[1]) - self.kernel_size + 1\n            flat_sz_w           = self.kernel_size[0] - self.kernel_size + 1\n            self.flat_sz        = flat_sz_h * flat_sz_w * self.out_channels\n            self.fc             = torch.nn.Linear(self.flat_sz, self.kernel_size)\n    \n    def concat(self, e1_embed, rel_embed):\n        e1_embed    = e1_embed. view(-1, 1, self.p.embed_dim)\n        rel_embed   = rel_embed.view(-1, 1, self.p.embed_dim)\n        stack_inp   = torch.cat([e1_embed, rel_embed], 1)\n        stack_inp   = torch.transpose(stack_inp, 2, 1).reshape((-1, 1, 2*self.p.k_w, self.p.k_h))\n        return stack_inp\n    \n    def projected(self, ent, norm):\n        norm = F.normalize(norm, p=2, dim=-1)\n        return ent - torch.sum(ent * norm, dim = 1, keepdim=True) * norm\n    \n    def forward_score(self):\n        if   self.func_type == \"transe\":\n            x        = self.sub_emb + self.rel_emb - self.obj_emb\n        elif self.func_type == \"transh\":\n            head       = self.entity_embedding(self.sub_emb)\n            tail       = self.entity_embedding(self.obj_emb)\n            r_norm     = self.relation_norm_embedding(self.rel_emb)\n            r_hyper    = self.relation_hyper_embedding(self.rel_emb)\n            head_hyper = self.projected(head, r_norm)\n            tail_hyper = self.projected(tail, r_norm)\n            x          = torch.norm(head_hyper + r_hyper - tail_hyper, p=2, dim=2)\n        elif self.func_type == \"distmult\":\n            x        =   torch.mm(self.sub_emb + self.rel_emb, self.obj_emb.transpose(1, 0))\n            x        +=  self.bias.expand_as(x)\n        elif self.func_type == \"conve\":\n            stk_inp  = self.concat(sub_emb, rel_emb)\n            x        = self.bn0(stk_inp)\n            x        = self.m_conv1(x)\n            x        = self.bn1(x)\n            x        = F.relu(x)\n            x        = self.feature_drop(x)\n            x        = x.view(-1, self.flat_sz)\n            x        = self.fc(x)\n            x        = self.hidden_drop2(x)\n            x        = self.bn2(x)\n            x        = F.relu(x)\n\n            x = torch.mm(x, self.obj_emb.transpose(1,0))\n            x += self.bias.expand_as(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:55:36.257904Z","iopub.execute_input":"2022-04-30T11:55:36.258097Z","iopub.status.idle":"2022-04-30T11:55:36.285055Z","shell.execute_reply.started":"2022-04-30T11:55:36.258062Z","shell.execute_reply":"2022-04-30T11:55:36.284281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CompGcn_with_temporal(nn.Module):\n    def __init__(self, conv_dim, num_layer ,num_relation, num_entity,node_dim, num_basis_vector, edge_idx, edge_type, num_class,score_func=\"TransE\"):\n        '''\n        Notice that in preprocessing, we assume that the node number will not be changed on the graph, only change relation.\n        input params:\n            1. conv_dim, (in_channel, out_channel)\n            2. num_layer, number of CompGCN layer, now assume to 2 per graph\n            3. num_relation, the number of relations_type for each graph(after preprocessing, should be the same for each graph)\n            4. num_entity, number of entity, should be the same for each graph\n            5. node_dimension, dimension of nodes\n            6. num_basis_vector, the first layer basis of first graph.\n            7. edge_idx, a list of edge_idx\n            8. edge_type, a list of edge_type\n            9. num_class, class type\n        '''\n        super().__init__()\n        self.node_features = get_param(shape= (num_entity,node_dim))\n        assert node_dim == conv_dim[0]\n        self.conv1 = CompGcn_total(conv_dim[0], conv_dim[1], num_relation, num_basis_vector, edge_idx[0],edge_type[0],True)\n        self.conv2 = CompGcn_total(conv_dim[1], conv_dim[1], num_relation, num_basis_vector, edge_idx[1],edge_type[1],False)\n        self.conv3 = CompGcn_total(conv_dim[1], conv_dim[1], num_relation, num_basis_vector, edge_idx[2],edge_type[2],False)\n        # change bert input to same as before\n        self.ln1 = nn.Linear(conv_dim[1], num_class)\n        self.dropout_node = nn.Dropout(0.2)\n        self.dropout_rel = nn.Dropout(0.2)\n        self.ln1 = self.func_init(self.ln1)\n        self.score = score_func\n    def func_init(self,m):\n        if type(m) == nn.Linear:\n            nn.init.xavier_uniform_(m.weight)\n        return m\n    def forward(self, head_index, tail_index, rel_index):\n        '''\n        Node index and rel index are corresponding information in a batch for bert part, we only care about the node, edge relation in a batch.\n        Since the embedding is tail - relation to head, the source will be tail, target will be head\n        '''\n        with amp.autocast():\n            device = self.node_features.device\n            node_embd1, rel_embd1 = self.conv1(self.node_features,device = device ) \n            node_embd2, rel_embd2 = self.conv2(node_embeding = node_embd1, rel_embeding = rel_embd1,device = device)\n            node_embd2, rel_embd2 = self.dropout_node(node_embd2), self.dropout_rel(rel_embd2) \n            node_embd3, rel_embd3 = self.conv3(node_embeding = node_embd2, rel_embeding = rel_embd2,device = device)\n            # then choose corresponding index out:\n            # shape should be (len(index), hidden_out)\n            hidden_node_state = node_embd3[tail_index,:]\n            hidden_rel_state  = rel_embd3[rel_index,:]\n            hidden_target_state = node_embd3[head_index,:]\n            head, rel, tail      = (\n                                        hidden_node_state, \n                                        hidden_rel_state, \n                                        hidden_target_state\n                                   )\n            score                = Score_func(head, rel, tail, func_type=self.score)\n            score                = score.forward_score()\n            score = self.ln1(score)\n        return score # hidden_node_state, hidden_rel_state, hidden_target_state","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:55:36.286967Z","iopub.execute_input":"2022-04-30T11:55:36.287166Z","iopub.status.idle":"2022-04-30T11:55:36.301528Z","shell.execute_reply.started":"2022-04-30T11:55:36.287141Z","shell.execute_reply":"2022-04-30T11:55:36.300883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class language_Dataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        '''\n        df is dataframe given previously\n        '''\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        '''\n        This function will return the index\n        '''\n        return torch.tensor(self.df.iloc[idx][\"labels\"]), self.df.iloc[idx][\"index_where\"]","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:55:36.302917Z","iopub.execute_input":"2022-04-30T11:55:36.303485Z","iopub.status.idle":"2022-04-30T11:55:36.312019Z","shell.execute_reply.started":"2022-04-30T11:55:36.303448Z","shell.execute_reply":"2022-04-30T11:55:36.311256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def try_gpu(i=0):\n    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n    if torch.cuda.device_count() >= i + 1:\n        return torch.device(f'cuda:{i}')\n    return torch.device('cpu')\n\n# In[ ]:\ndef train_with_amp(net, train_set, criterion, optimizer, epochs,batch_size, scheduler, gradient_accumulate_step, max_grad_norm , num_gpu):\n    net.train()\n    # instantiate a scalar object \n    ls = []\n    device = [try_gpu(i) for i in range(num_gpu)]\n    print(\"train on \" + str(device))\n    enable_amp = True if \"cuda\" in device[0].type else False\n    scaler = amp.GradScaler(enabled= enable_amp)\n    net.to(device[0])\n    global_step = 0\n    train_iter = torch.utils.data.DataLoader(train_set, batch_size = batch_size)\n    for epoch in range(epochs):\n        for idx, value in enumerate(train_iter):\n            labels, index = value\n            labels = labels.to(device[0])\n            head_values = torch.tensor(index[0]).to(device[0])\n            tail_values = torch.tensor(index[2]).to(device[0])\n            rel_values = torch.tensor(index[1]).to(device[0])\n            # when forward process, use amp\n            with amp.autocast(enabled= enable_amp):\n                output = net(head_values,tail_values, rel_values)  \n            loss = criterion(output, labels.view(-1,1).float())\n            # prevent gradient to 0\n            if gradient_accumulate_step > 1:\n                # 如果显存不足，通过 gradient_accumulate 来解决\n                loss = loss/gradient_accumulate_step\n            \n            # 放大梯度，避免其消失\n            scaler.scale(loss).backward()\n            # do the gradient clip\n            gradient_norm = nn.utils.clip_grad_norm_(net.parameters(),max_grad_norm)\n            if (idx + 1) % gradient_accumulate_step == 0:\n                # 多少 step 更新一次梯度\n                # 通过 scaler.step 来unscale 回梯度值， 如果气结果不是infs 和Nans， 调用optimizer.step()来更新权重\n                # 否则忽略step调用， 保证权重不更新\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                global_step += 1\n                scheduler.step()\n            # 每100次计算 print 出一次loss\n            if idx % 1000 == 0 or idx == len(train_iter) -1:\n                with torch.no_grad():\n                    print(\"==============Epochs \"+ str(epoch) + \" ======================\")\n                    print(\"loss: \" + str(loss) + \"; grad_norm: \" + str(gradient_norm))\n                torch.save({'epoch': epoch,\n                'model_state_dict': net.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': loss},\"./checkpoint.params\")\n                ls.append(loss.item())\n                #print(\"successfully done one train\")\n    import matplotlib.pyplot as plt\n    plt.figure()\n    plt.plot(ls)\n    \n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:55:36.313384Z","iopub.execute_input":"2022-04-30T11:55:36.313884Z","iopub.status.idle":"2022-04-30T11:55:36.330043Z","shell.execute_reply.started":"2022-04-30T11:55:36.313846Z","shell.execute_reply":"2022-04-30T11:55:36.329288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    edge_idx_1 = torch.tensor(np.load(\"../input/3-graphs-info/edge_idx0.npz\")[\"arr_0\"])\n    edge_idx_2 = torch.tensor(np.load(\"../input/3-graphs-info/edge_idx1.npz\")[\"arr_0\"])\n    edge_idx_3 = torch.tensor(np.load(\"../input/3-graphs-info/edge_idx2.npz\")[\"arr_0\"])\n    num_nodes_0 = torch.tensor(np.load(\"../input/3-graphs-info/graph_0_num_nodes.npz\")[\"arr_0\"])\n    num_nodes_1 = torch.tensor(np.load(\"../input/3-graphs-info/graph_1_num_nodes.npz\")[\"arr_0\"])\n    num_nodes_2 = torch.tensor(np.load(\"../input/3-graphs-info/graph_2_num_nodes.npz\")[\"arr_0\"])\n    num_relation_0 = torch.tensor(np.load(\"../input/3-graphs-info/graph_0_num_edges.npz\")[\"arr_0\"])\n    num_relation_1 = torch.tensor(np.load(\"../input/3-graphs-info/graph_1_num_edges.npz\")[\"arr_0\"])\n    num_relation_2 = torch.tensor(np.load(\"../input/3-graphs-info/graph_2_num_edges.npz\")[\"arr_0\"])\n    edge_type_0 = torch.tensor(np.load(\"../input/3-graphs-info/edge_type0.npz\")[\"arr_0\"])\n    edge_type_1 = torch.tensor(np.load(\"../input/3-graphs-info/edge_type1.npz\")[\"arr_0\"])\n    edge_type_2 = torch.tensor(np.load(\"../input/3-graphs-info/edge_type2.npz\")[\"arr_0\"])\n    head2idx = np.load('../input/3-graphs-info/graph_2entity2index.npy', allow_pickle=True).item()\n    rel2idx =  np.load('../input/3-graphs-info/graph_2rel2index.npy', allow_pickle=True).item()\n    conv_dim, num_layer, node_dim, num_basis, edge_idx, edge_type = [64, 128], 2, 64, 37, [edge_idx_1,edge_idx_2,edge_idx_3],[edge_type_0,edge_type_1,edge_type_2]\n    tmp = CompGcn_with_temporal(conv_dim,num_layer, num_relation_2, num_nodes_2+1, node_dim, num_basis,edge_idx, edge_type, 1)\n    train = pd.read_csv(\"../input/train-valid-test-dataset/train.csv\").drop(\"Unnamed: 0\", axis = 1)\n    train[\"index_where\"] = train[\"index_where\"].apply(ast.literal_eval)\n    train_set = language_Dataset(train)\n    loss = nn.BCEWithLogitsLoss()\n    batch_size = 2\n    lr = 2e-6\n    num_gpu = 1\n    optimizer = torch.optim.AdamW(tmp.parameters(), lr = lr)\n    scheduler = get_cosine_schedule_with_warmup(optimizer= optimizer, num_warmup_steps = 0, num_training_steps= len(torch.utils.data.DataLoader(train_set, batch_size = batch_size)), num_cycles = 0.5)\n    train_with_amp(tmp, train_set, loss,optimizer,3,batch_size, scheduler,1,1000, num_gpu)","metadata":{"execution":{"iopub.status.busy":"2022-04-30T11:55:36.333617Z","iopub.execute_input":"2022-04-30T11:55:36.33441Z","iopub.status.idle":"2022-04-30T11:55:43.927108Z","shell.execute_reply.started":"2022-04-30T11:55:36.334376Z","shell.execute_reply":"2022-04-30T11:55:43.926315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}